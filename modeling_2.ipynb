{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dN6adyMwl2S"
   },
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtyTRglturFs",
    "outputId": "9157fd9e-e2e2-4da2-c402-15e8b11de480"
   },
   "outputs": [],
   "source": [
    "#!pip install skimpy polars plotly pingouin xgboost lightgbm scikit-optimize optuna category_encoders sklego\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from polars import DataFrame\n",
    "from skimpy import skim\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.io as pio\n",
    "import polars.selectors as cs\n",
    "import pingouin as pg\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skopt import BayesSearchCV\n",
    "from polars import DataFrame\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.linear_model._logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from optuna.trial._frozen import FrozenTrial\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    make_scorer,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    fbeta_score,\n",
    "    roc_auc_score,\n",
    "    make_scorer,\n",
    "    matthews_corrcoef,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble._forest import ExtraTreesClassifier\n",
    "\n",
    "from optuna import Study\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from optuna import Trial,create_study\n",
    "\n",
    "from category_encoders import WOEEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from optuna import create_study\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "from optuna.samplers import RandomSampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xyoCfFXHuwLb"
   },
   "outputs": [],
   "source": [
    "def calculate_partial_correlation(df, feature1, feature2, control):\n",
    "    return pg.partial_corr(\n",
    "        data=df.to_pandas(), x=feature1, y=feature2, covar=control\n",
    "    )  # ['r'].values[0]\n",
    "\n",
    "\n",
    "def create_encoder_mapping(df, feature) -> dict[str, int]:\n",
    "    \"\"\"Creates dictionary for mapping to encode categorical features\n",
    "\n",
    "    Args:\n",
    "        df (polars dataframe): dataframe of features\n",
    "        feature (string): name of feature of interest\n",
    "\n",
    "    Returns:\n",
    "        encoding_key: dictionary of feature values and numbers for encoding\n",
    "    \"\"\"\n",
    "    df: DataFrame = (\n",
    "        df.group_by(feature)\n",
    "        .agg(pl.len().alias(\"values\"))\n",
    "        .sort(\"values\", descending=True)\n",
    "    )\n",
    "\n",
    "    options: List = df[feature].to_list()\n",
    "\n",
    "    numbers_to_encode = list(range(0, len(options)))\n",
    "    encoding_key = {options[i]: numbers_to_encode[i] for i in range(len(options))}\n",
    "\n",
    "    if df[feature].str.contains(\"Yes\").to_list()[0] == True:\n",
    "        encoding_key: dict[str, int] = {\"Yes\": 1, \"No\": 0}\n",
    "\n",
    "    return encoding_key\n",
    "\n",
    "\n",
    "def encode_feature(df, feature, encoding_key) -> DataFrame:\n",
    "    \"\"\"Encode features using supplied encoding key\n",
    "\n",
    "    Args:\n",
    "        df (polars): Dataframe to be modified\n",
    "        feature (string): feature to be encoded\n",
    "        encoding_key (dict): dictionary of values and numerical codes\n",
    "\n",
    "    Returns:\n",
    "        df: input dataframe with feature replaced by numerical values\n",
    "    \"\"\"\n",
    "    df: DataFrame = df.with_columns(\n",
    "        df.select(pl.col(feature).replace(encoding_key)).cast({feature: pl.Int64})\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def retrieve_csv_columns(csv):\n",
    "    df = pl.read_csv(csv).head(1)\n",
    "    columns = set(lower_column_names(df).columns)\n",
    "    return columns\n",
    "\n",
    "\n",
    "def find_common_key(table1, table2) -> str:\n",
    "    \"\"\"For two names of csv files given as strings, return the common key column between them\"\"\"\n",
    "    sk_id_set = set(\n",
    "        [\n",
    "            \"bureau.csv\",\n",
    "            \"previous_application.csv\",\n",
    "            \"credit_card_balance.csv\",\n",
    "            \"installments_payments.csv\",\n",
    "            \"pos_cash_balance.csv\",\n",
    "            \"application_test.csv\",\n",
    "            \"application_train.csv\",\n",
    "        ]\n",
    "    )\n",
    "    sk_id_bureau_set = set([\"bureau.csv\", \"bureau_balance.csv\"])\n",
    "    sk_id_prev_set = set(\n",
    "        [\n",
    "            \"pos_cash_balance.csv\",\n",
    "            \"installments_payments.csv\",\n",
    "            \"credit_card_balance.csv\",\n",
    "            \"previous_application.csv\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if (table1 in sk_id_set) & (table2 in sk_id_set):\n",
    "        return \"sk_id_curr\"\n",
    "    elif (table1 in sk_id_bureau_set) & (table2 in sk_id_bureau_set):\n",
    "        return \"sk_id_bureau\"\n",
    "    elif (table1 in sk_id_prev_set) & (table2 in sk_id_set):\n",
    "        return \"sk_id_prev\"\n",
    "    else:\n",
    "        print(\"no common key found for these tables\")\n",
    "\n",
    "\n",
    "def tables_with_feature(description_df, feature):\n",
    "    tables = set(\n",
    "        description_df.filter(pl.col(\"row\") == feature)\n",
    "        .select(\"table\")\n",
    "        .unique()\n",
    "        .to_series()\n",
    "    )\n",
    "\n",
    "    if len(tables) == 0:\n",
    "        print(\"no results found.\")\n",
    "    else:\n",
    "        return tables\n",
    "\n",
    "\n",
    "def calculate_null_count(train, table, feature, mode=\"null_count\"):\n",
    "    #nulls = []\n",
    "    if table == \"application_{train|test}.csv\":\n",
    "        if mode == \"null_count\":\n",
    "            return train[feature].null_count()\n",
    "        elif mode == \"series\":\n",
    "            return train[feature]\n",
    "        # nulls.append(train[feature].null_count())\n",
    "    else:\n",
    "        alternate_df = create_formatted_df(table)\n",
    "        key = find_common_key(\"application_train.csv\", table)\n",
    "        temp_df = train.join(alternate_df, on=key, how=\"inner\")\n",
    "    if mode == \"null_count\":\n",
    "        return temp_df[feature].null_count()\n",
    "    elif mode == \"series\":\n",
    "        return temp_df[feature].to_series()\n",
    "\n",
    "\n",
    "def null_count_comparison(train, description, feature):\n",
    "    tables = tables_with_feature(description, feature)\n",
    "\n",
    "    nulls = []\n",
    "    for table in tables:\n",
    "        null_count = calculate_null_count(train, table, feature, mode=\"null_count\")\n",
    "        nulls.append(null_count)\n",
    "    df = pl.DataFrame(\n",
    "        data={\"table\": list(tables), \"feature\": feature, \"null_count\": nulls},\n",
    "        schema={\"table\": pl.String, \"feature\": pl.String, \"null_count\": pl.Int64},\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_feature(train, null_df, feature) -> DataFrame:\n",
    "    \"\"\"Replace feature with fewer null values in train, else return training set unchanged\"\"\"\n",
    "    null_min = null_df[\"null_count\"].min()\n",
    "    table = null_df.filter(pl.col(\"null_count\") == null_min).select(\"table\").item()\n",
    "    if table == \"application_{train|test}.csv\":\n",
    "        return train\n",
    "    else:\n",
    "        key = find_common_key(\"application_train.csv\", table)\n",
    "        alternate_df = create_formatted_df(table)[[key, feature]]\n",
    "        alternate_feature = train.join(alternate_df, on=key, how=\"inner\")[\n",
    "            feature\n",
    "        ].to_series()\n",
    "        train = train.with_columns(alternate_feature.alias(feature))\n",
    "        return train\n",
    "\n",
    "\n",
    "def hypothesis_test_multiple_proportions(train_df, feature):\n",
    "    \"\"\"Conducts hypothesis test comparing target proportion and feature proportion\"\"\"\n",
    "\n",
    "    # Determine the number of values in feature\n",
    "    feature_default_df = pd.crosstab(\n",
    "        train_df[feature].to_pandas(),\n",
    "        train_df[\"target\"].to_pandas(),\n",
    "        rownames=[feature],\n",
    "        colnames=[\"target\"],\n",
    "    )\n",
    "    feature_default_df[\"default_proportion\"] = feature_default_df.iloc[\n",
    "        :, 1\n",
    "    ] / feature_default_df.sum(axis=1)\n",
    "    feature_default_df[\"total\"] = feature_default_df.iloc[:, :-1].sum(axis=1)\n",
    "\n",
    "    # pooled sample proportion\n",
    "    p1_default_prop = feature_default_df.default_proportion.iloc[0]\n",
    "    p2_default_prop = feature_default_df.default_proportion.iloc[1]\n",
    "    p1_population = feature_default_df.total.iloc[0]\n",
    "    p2_population = feature_default_df.total.iloc[1]\n",
    "\n",
    "    p = (p1_default_prop * p1_population + p2_default_prop * p2_population) / (\n",
    "        p1_population + p2_population\n",
    "    )\n",
    "\n",
    "    # standard error\n",
    "    se = np.sqrt((p * (1 - p)) * ((1 / p1_population) + (1 / p2_population)))\n",
    "\n",
    "    # test statistic\n",
    "    z = (p1_default_prop - p2_default_prop) / se\n",
    "\n",
    "    if np.abs(z) < 1.64485:\n",
    "        print(\n",
    "            f\"Fail to reject the null hypothesis. We can assume the default percentage to be the same across {feature}.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"z = {z:.3f}. Reject null hypothesis. The proportion of credit defaults across values of {feature} is not equal.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def np_to_df(array, feature_list, designation=\"pd\"):\n",
    "    if designation == \"pd\":\n",
    "        return pd.DataFrame(array, columns=feature_list)\n",
    "    if designation == \"pl\":\n",
    "        df = pl.DataFrame(array)\n",
    "        df.columns = feature_list\n",
    "        return df\n",
    "\n",
    "\n",
    "def conduct_grid_search_tuning(\n",
    "    model, grid, x_train, y_train, refit, scoring=make_scorer(fbeta_score, beta=2), cv=5\n",
    "):\n",
    "    \"\"\"Conducts gridsearch for specified model and hyperparameter settings\n",
    "\n",
    "    Args:\n",
    "        model (string): string specifying model to test, must be 'knn', 'logistic_regression','decision_tree', or 'random_forest'\n",
    "        grid (dictionary): grid of lists specifying options for hyperparameters to tune\n",
    "        xy (list): x and y for model fitting, should be in [x_train,y_train] format\n",
    "        scoring(string/callable): string defines scoring method to be used within grid search\n",
    "    \"\"\"\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, grid, cv=cv, scoring=scoring, refit=refit, n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    return best_params  # , grid_search\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def skopt_bayesian_search(classifier, x_train, y_train, params):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "    search = BayesSearchCV(estimator=classifier, search_spaces=params, n_jobs=-1, cv=cv)\n",
    "    search.fit(x_train, y_train)\n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HDD8WUY6urFs"
   },
   "outputs": [],
   "source": [
    "from loan_functions import (\n",
    "    create_formatted_df,\n",
    "    make_subplot,\n",
    "    lower_column_names,\n",
    "    lower_column_values,\n",
    "    column_description,\n",
    "    plot_histogram,\n",
    "    column_comparison,\n",
    "    int_range,\n",
    "    clear,\n",
    "    calculate_value_counts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCapjrYUurFs"
   },
   "source": [
    "We'll read in the csv datasets one by one to save on memory, clearing them each time. Let's start with the training set, followed by our other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the training set and looking at balance. I'll keep the EDA of this notebook focused on the training set to have a more focused analysis and discussion of modeling and model performance, but a more detailed exploration of the supporting datasets can be found in the [notebook] found in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 307511 rows and 122 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 122)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sk_id_curr</th><th>target</th><th>name_contract_type</th><th>code_gender</th><th>flag_own_car</th><th>flag_own_realty</th><th>cnt_children</th><th>amt_income_total</th><th>amt_credit</th><th>amt_annuity</th><th>amt_goods_price</th><th>name_type_suite</th><th>name_income_type</th><th>name_education_type</th><th>name_family_status</th><th>name_housing_type</th><th>region_population_relative</th><th>days_birth</th><th>days_employed</th><th>days_registration</th><th>days_id_publish</th><th>own_car_age</th><th>flag_mobil</th><th>flag_emp_phone</th><th>flag_work_phone</th><th>flag_cont_mobile</th><th>flag_phone</th><th>flag_email</th><th>occupation_type</th><th>cnt_fam_members</th><th>region_rating_client</th><th>region_rating_client_w_city</th><th>weekday_appr_process_start</th><th>hour_appr_process_start</th><th>reg_region_not_live_region</th><th>reg_region_not_work_region</th><th>live_region_not_work_region</th><th>&hellip;</th><th>nonlivingarea_medi</th><th>fondkapremont_mode</th><th>housetype_mode</th><th>totalarea_mode</th><th>wallsmaterial_mode</th><th>emergencystate_mode</th><th>obs_30_cnt_social_circle</th><th>def_30_cnt_social_circle</th><th>obs_60_cnt_social_circle</th><th>def_60_cnt_social_circle</th><th>days_last_phone_change</th><th>flag_document_2</th><th>flag_document_3</th><th>flag_document_4</th><th>flag_document_5</th><th>flag_document_6</th><th>flag_document_7</th><th>flag_document_8</th><th>flag_document_9</th><th>flag_document_10</th><th>flag_document_11</th><th>flag_document_12</th><th>flag_document_13</th><th>flag_document_14</th><th>flag_document_15</th><th>flag_document_16</th><th>flag_document_17</th><th>flag_document_18</th><th>flag_document_19</th><th>flag_document_20</th><th>flag_document_21</th><th>amt_req_credit_bureau_hour</th><th>amt_req_credit_bureau_day</th><th>amt_req_credit_bureau_week</th><th>amt_req_credit_bureau_mon</th><th>amt_req_credit_bureau_qrt</th><th>amt_req_credit_bureau_year</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>&hellip;</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>311717</td><td>0</td><td>&quot;cash loans&quot;</td><td>&quot;m&quot;</td><td>&quot;n&quot;</td><td>&quot;n&quot;</td><td>0</td><td>67500.0</td><td>161730.0</td><td>8257.5</td><td>135000.0</td><td>&quot;unaccompanied&quot;</td><td>&quot;working&quot;</td><td>&quot;secondary / secondary special&quot;</td><td>&quot;civil marriage&quot;</td><td>&quot;house / apartment&quot;</td><td>0.01452</td><td>-11548</td><td>-113</td><td>-6156.0</td><td>-2596</td><td>null</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>&quot;laborers&quot;</td><td>2.0</td><td>2</td><td>2</td><td>&quot;saturday&quot;</td><td>15</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-2452.0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.0</td></tr><tr><td>121800</td><td>0</td><td>&quot;cash loans&quot;</td><td>&quot;f&quot;</td><td>&quot;n&quot;</td><td>&quot;y&quot;</td><td>1</td><td>90000.0</td><td>148500.0</td><td>17752.5</td><td>148500.0</td><td>&quot;spouse, partner&quot;</td><td>&quot;commercial associate&quot;</td><td>&quot;secondary / secondary special&quot;</td><td>&quot;married&quot;</td><td>&quot;house / apartment&quot;</td><td>0.028663</td><td>-14634</td><td>-874</td><td>-5078.0</td><td>-3135</td><td>null</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>&quot;core staff&quot;</td><td>3.0</td><td>2</td><td>2</td><td>&quot;monday&quot;</td><td>14</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0.0</td><td>null</td><td>&quot;block of flats&quot;</td><td>0.1569</td><td>&quot;stone, brick&quot;</td><td>&quot;no&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-704.0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3.0</td></tr><tr><td>136700</td><td>0</td><td>&quot;cash loans&quot;</td><td>&quot;m&quot;</td><td>&quot;y&quot;</td><td>&quot;y&quot;</td><td>1</td><td>180000.0</td><td>360000.0</td><td>22023.0</td><td>360000.0</td><td>&quot;unaccompanied&quot;</td><td>&quot;state servant&quot;</td><td>&quot;incomplete higher&quot;</td><td>&quot;married&quot;</td><td>&quot;house / apartment&quot;</td><td>0.04622</td><td>-10083</td><td>-1036</td><td>-2019.0</td><td>-2726</td><td>1.0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>&quot;core staff&quot;</td><td>3.0</td><td>1</td><td>1</td><td>&quot;friday&quot;</td><td>11</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0.0</td><td>&quot;reg oper account&quot;</td><td>&quot;block of flats&quot;</td><td>0.0097</td><td>&quot;stone, brick&quot;</td><td>&quot;no&quot;</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>-460.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td></tr><tr><td>134033</td><td>0</td><td>&quot;cash loans&quot;</td><td>&quot;m&quot;</td><td>&quot;y&quot;</td><td>&quot;n&quot;</td><td>1</td><td>157500.0</td><td>490495.5</td><td>27387.0</td><td>454500.0</td><td>&quot;unaccompanied&quot;</td><td>&quot;working&quot;</td><td>&quot;secondary / secondary special&quot;</td><td>&quot;separated&quot;</td><td>&quot;with parents&quot;</td><td>0.006671</td><td>-13769</td><td>-268</td><td>-7186.0</td><td>-4677</td><td>19.0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>&quot;low-skill laborers&quot;</td><td>2.0</td><td>2</td><td>2</td><td>&quot;tuesday&quot;</td><td>12</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>-813.0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>267388</td><td>0</td><td>&quot;revolving loans&quot;</td><td>&quot;f&quot;</td><td>&quot;y&quot;</td><td>&quot;n&quot;</td><td>0</td><td>90000.0</td><td>270000.0</td><td>13500.0</td><td>270000.0</td><td>&quot;unaccompanied&quot;</td><td>&quot;pensioner&quot;</td><td>&quot;secondary / secondary special&quot;</td><td>&quot;single / not married&quot;</td><td>&quot;house / apartment&quot;</td><td>0.018634</td><td>-23627</td><td>365243</td><td>-15919.0</td><td>-4376</td><td>1.0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>null</td><td>1.0</td><td>2</td><td>2</td><td>&quot;saturday&quot;</td><td>10</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0.0005</td><td>&quot;org spec account&quot;</td><td>&quot;block of flats&quot;</td><td>0.2339</td><td>&quot;panel&quot;</td><td>&quot;no&quot;</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>-327.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>6.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 122)\n",
       "┌────────────┬────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ sk_id_curr ┆ target ┆ name_contr ┆ code_gend ┆ … ┆ amt_req_c ┆ amt_req_c ┆ amt_req_c ┆ amt_req_c │\n",
       "│ ---        ┆ ---    ┆ act_type   ┆ er        ┆   ┆ redit_bur ┆ redit_bur ┆ redit_bur ┆ redit_bur │\n",
       "│ i64        ┆ i64    ┆ ---        ┆ ---       ┆   ┆ eau_week  ┆ eau_mon   ┆ eau_qrt   ┆ eau_year  │\n",
       "│            ┆        ┆ str        ┆ str       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆        ┆            ┆           ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64       │\n",
       "╞════════════╪════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 311717     ┆ 0      ┆ cash loans ┆ m         ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 5.0       │\n",
       "│ 121800     ┆ 0      ┆ cash loans ┆ f         ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 3.0       │\n",
       "│ 136700     ┆ 0      ┆ cash loans ┆ m         ┆ … ┆ 0.0       ┆ 1.0       ┆ 0.0       ┆ 1.0       │\n",
       "│ 134033     ┆ 0      ┆ cash loans ┆ m         ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
       "│ 267388     ┆ 0      ┆ revolving  ┆ f         ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 6.0       │\n",
       "│            ┆        ┆ loans      ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_train = create_formatted_df(\"application_train.csv\")\n",
    "\n",
    "print(\n",
    "    f\"Training set has {application_train.shape[0]} rows and {application_train.shape[1]} columns.\"\n",
    ")\n",
    "\n",
    "application_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at balance of the target variable since that is our modeling variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "customdata": [
          [
           "91.9%"
          ],
          [
           "8.1%"
          ]
         ],
         "hovertemplate": "target=%{marker.color}<br>count=%{y}<br>percentages=%{customdata[0]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           0,
           1
          ],
          "coloraxis": "coloraxis",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          "91.9%",
          "8.1%"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          0,
          1
         ],
         "xaxis": "x",
         "y": [
          282686,
          24825
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "bargap": 0.2,
        "barmode": "relative",
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "target"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(210, 251, 212)"
          ],
          [
           0.16666666666666666,
           "rgb(165, 219, 194)"
          ],
          [
           0.3333333333333333,
           "rgb(123, 188, 176)"
          ],
          [
           0.5,
           "rgb(85, 156, 158)"
          ],
          [
           0.6666666666666666,
           "rgb(58, 124, 137)"
          ],
          [
           0.8333333333333334,
           "rgb(35, 93, 114)"
          ],
          [
           1,
           "rgb(18, 63, 90)"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Target Variable Distribution"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "target"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(application_train, \"target\", title=\"Target Variable Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>table</th><th>feature</th><th>null_count</th></tr><tr><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;bureau.csv&quot;</td><td>&quot;amt_annuity&quot;</td><td>42</td></tr><tr><td>&quot;application_{train|test}.csv&quot;</td><td>&quot;amt_annuity&quot;</td><td>12</td></tr><tr><td>&quot;previous_application.csv&quot;</td><td>&quot;amt_annuity&quot;</td><td>93</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 3)\n",
       "┌──────────────────────────────┬─────────────┬────────────┐\n",
       "│ table                        ┆ feature     ┆ null_count │\n",
       "│ ---                          ┆ ---         ┆ ---        │\n",
       "│ str                          ┆ str         ┆ i64        │\n",
       "╞══════════════════════════════╪═════════════╪════════════╡\n",
       "│ bureau.csv                   ┆ amt_annuity ┆ 42         │\n",
       "│ application_{train|test}.csv ┆ amt_annuity ┆ 12         │\n",
       "│ previous_application.csv     ┆ amt_annuity ┆ 93         │\n",
       "└──────────────────────────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = pl.read_csv(\"HomeCredit_columns_description.csv\", encoding=\"latin1\")\n",
    "description = lower_column_names(description)\n",
    "description = lower_column_values(description)\n",
    "\n",
    "# Delete training spaces for some entries in the row column\n",
    "description = description.with_columns(\n",
    "    pl.col(\"row\").map_elements(lambda x: x.split(\" \")[0], return_dtype=pl.String)\n",
    ")\n",
    "\n",
    "null_df = null_count_comparison(application_train, description, \"amt_annuity\")\n",
    "null_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "percentage=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "texttemplate": "%{value}",
         "type": "histogram",
         "x": [
          0.6987229585647583,
          0.6987229585647583,
          0.6987229585647583,
          0.6943296194076538,
          0.6943296194076538,
          0.6943296194076538,
          0.6838617324829102,
          0.6835495233535767,
          0.6835495233535767,
          0.6835495233535767,
          0.6784862875938416,
          0.6784862875938416,
          0.6784862875938416,
          0.6649778485298157,
          0.6649778485298157,
          0.6649778485298157,
          0.6599081158638,
          0.5937674045562744,
          0.5937674045562744,
          0.5937674045562744,
          0.5851595401763916,
          0.5851595401763916,
          0.5851595401763916,
          0.5638107061386108,
          0.5517916679382324,
          0.5517916679382324,
          0.5517916679382324,
          0.5329598188400269,
          0.5329598188400269,
          0.5329598188400269,
          0.5084078311920166,
          0.5074973106384277,
          0.5074973106384277,
          0.5074973106384277,
          0.5034877061843872,
          0.5034877061843872,
          0.5034877061843872,
          0.5019332766532898,
          0.5019332766532898,
          0.5019332766532898,
          0.5017609000205994,
          0.4976082146167755,
          0.4976082146167755,
          0.4976082146167755,
          0.4878101944923401,
          0.4878101944923401,
          0.4878101944923401,
          0.4826851785182953,
          0.4739830493927002,
          0.3134554624557495,
          0.1982530653476715,
          0.1350163072347641,
          0.1350163072347641,
          0.1350163072347641,
          0.1350163072347641,
          0.1350163072347641,
          0.1350163072347641,
          0.004201475530862808,
          0.0033202064223587513,
          0.0033202064223587513,
          0.0033202064223587513,
          0.0033202064223587513,
          0.002146264770999551,
          0.0009040327277034521,
          0.00003902299431501888,
          0.00000650383253741893,
          0.000003251916268709465,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "bargap": 0.2,
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Null value percentages across dataset"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "percentage"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "null_df = (\n",
    "    application_train.null_count()\n",
    "    .transpose(include_header=True)\n",
    "    .rename(mapping={\"column\": \"feature\", \"column_0\": \"null_count\"})\n",
    "    .sort(by=\"null_count\", descending=True)\n",
    "    .with_columns(\n",
    "        pl.col(\"null_count\")\n",
    "        .map_elements(lambda x: x / len(application_train), return_dtype=pl.Float32)\n",
    "        .alias(\"percentage\")\n",
    "    )\n",
    ")\n",
    "\n",
    "px.histogram(\n",
    "    null_df,\n",
    "    x=\"percentage\",\n",
    "    text_auto=True,\n",
    "    title=\"Null value percentages across dataset\",\n",
    ").update_layout(bargap=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows that we have a pretty wide spread of null value prevalence within this dataset. Thankfully none of these null values are in the target variable, but we will have to make a choice in our analysis of how to treat null values within this dataset since some classifiers are more tolerant of null values than others. LightGBM and XGBoost can handle null values when fitting, but commonly used sklearn classifiers do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for categorical anomalies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "target=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "histogram",
         "x": [
          -0.0021084690181960766,
          1,
          0.01918713359627062,
          -0.003981865601205793,
          -0.03036928646142988,
          -0.012816561515413605,
          -0.039645281169544896,
          -0.037227148542444764,
          0.07823930830982459,
          -0.04493166265773831,
          0.04197486283141728,
          0.05145717260705659,
          0.0376115642751661,
          0.0005343955790422479,
          0.04598221971659226,
          0.028524322363216906,
          0.00037012680234920295,
          -0.023806272330357217,
          -0.0017583834312544816,
          0.009307784396531512,
          0.058899014945713445,
          0.06089266756482329,
          -0.02416583143009371,
          0.005575944520908343,
          0.006941907545371869,
          0.002819479184159081,
          0.044395374805700945,
          0.05099446436812648,
          0.03251834110149831,
          -0.1553171260639496,
          -0.16047167160520875,
          -0.17891869762837048,
          -0.0294975646334316,
          -0.022745743190148507,
          -0.009727673268680202,
          -0.022149284063388727,
          -0.018549661629760343,
          -0.03419879367526856,
          -0.019172182012738025,
          -0.044003370532402865,
          -0.033613503251899074,
          -0.010884822678806584,
          -0.025030533291064243,
          -0.03299711746761117,
          -0.0031761075207260804,
          -0.013578069781984715,
          -0.02728387066775288,
          -0.01995228392431536,
          -0.009036447354301175,
          -0.02206820383485666,
          -0.016340337311304406,
          -0.03213117100499533,
          -0.017387422304882893,
          -0.04322626321381947,
          -0.03269782531047472,
          -0.010174103567660405,
          -0.023393245570197525,
          -0.030684615765641635,
          -0.001556560845903716,
          -0.01271054374887829,
          -0.029183758876693472,
          -0.022081261373189838,
          -0.009993096559696444,
          -0.022325926477474216,
          -0.018572868907710774,
          -0.033862876768446605,
          -0.019024756327399944,
          -0.04376792104770143,
          -0.033394287191135595,
          -0.011255826639923302,
          -0.02462066360650495,
          -0.03273928440781497,
          -0.0027571486021083394,
          -0.013336719980273935,
          -0.032595546758941996,
          0.0091306657027761,
          0.03224757925304534,
          0.009022143630215059,
          0.031276472126435764,
          0.055218483513459246,
          0.005417144279619328,
          0.044346346851144976,
          -0.0026720821701620937,
          -0.00031577741664638075,
          -0.02860189397306156,
          -0.0015195031600879073,
          -0.00804038446605664,
          -0.004352408580842042,
          -0.0014138915975241474,
          -0.004229349652160023,
          -0.0007557507043578883,
          -0.01158322234141947,
          -0.009463821935576941,
          -0.006535657584094894,
          -0.011614671588293178,
          -0.0033775691474138337,
          -0.007952385099462259,
          -0.001357518324300817,
          0.00021539355797734155,
          0.003708625029306583,
          0.0009304246096946755,
          0.0027044013780418243,
          0.000787711748903683,
          -0.012462419228138606,
          -0.0020219274353457245,
          0.019929858569467208
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "bargap": 0.2,
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Correlations with Target Variable"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "target"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any\n",
    "from numpy import dtype\n",
    "from numpy._typing._array_like import NDArray\n",
    "from pandas import Index\n",
    "from pandas.core.frame import DataFrame\n",
    "from pandas.io.formats.style import Styler\n",
    "\n",
    "\n",
    "corr: DataFrame = (\n",
    "    application_train.select(cs.by_dtype(pl.NUMERIC_DTYPES)).to_pandas().corr()\n",
    ")\n",
    "corr = pd.DataFrame(corr[\"target\"])\n",
    "\n",
    "px.histogram(\n",
    "    corr, \"target\", title=\"Correlations with Target Variable\"\n",
    ").update_layout(bargap=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows the distribution of correlations as a graphical distribution rather than in tabular form because we have so many variables that we're working with. None of our variables are correlated with target, save for target itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there difference in proportions of clients that default when broken down by their region of residence? \n",
    "$H_0$: $p_1$ = $p_2$ = $p_3$<br>\n",
    "$H_1$: $p_1$ $\\neq$ $p_2$ $\\neq$ $p_3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = -82.387. Reject null hypothesis. The proportion of credit defaults across values of region_rating_client is not equal.\n"
     ]
    }
   ],
   "source": [
    "region_default = pd.crosstab(\n",
    "    application_train[\"region_rating_client\"].to_pandas(),\n",
    "    application_train[\"target\"].to_pandas(),\n",
    "    rownames=[\"region_rating_client\"],\n",
    "    colnames=[\"target\"],\n",
    ")\n",
    "region_default[\"default_proportion\"] = region_default.iloc[:, 1] / region_default.sum(\n",
    "    axis=1\n",
    ")\n",
    "region_default[\"total\"] = region_default.iloc[:, :-1].sum(axis=1)\n",
    "\n",
    "# pooled sample proportion\n",
    "p1_default_prop = region_default.default_proportion.iloc[0]\n",
    "p2_default_prop = region_default.default_proportion.iloc[1]\n",
    "p3_default_prop = region_default.default_proportion.iloc[2]\n",
    "\n",
    "p1_population = region_default.total.iloc[0]\n",
    "p2_population = region_default.total.iloc[1]\n",
    "p3_population = region_default.total.iloc[2]\n",
    "\n",
    "p = (\n",
    "    p1_default_prop * p1_population\n",
    "    + p2_default_prop * p2_population\n",
    "    + p3_default_prop * p3_population\n",
    ") / (p1_population + p2_population + p3_population)\n",
    "\n",
    "# standard error\n",
    "se = np.sqrt(\n",
    "    (p * (1 - p)) * ((1 / p1_population) + (1 / p2_population) + (1 / p2_population))\n",
    ")\n",
    "\n",
    "# test statistic\n",
    "z = (p1_default_prop - p2_default_prop - p3_default_prop) / se\n",
    "\n",
    "if np.abs(z) < 1.64485:\n",
    "    print(\n",
    "        f\"Fail to reject the null hypothesis. We can assume the default percentage to be the same across {'region_rating_client'}.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"z = {z:.3f}. Reject null hypothesis. The proportion of credit defaults across values of {'region_rating_client'} is not equal.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because both gender and region are not equal across their values when compared to our target variable, we they are more likely to have a significant relationship with default risk. We should include these variables in our predictor pool. This raises the question of what variables we should use to predict our models, given that we feasibly cannot include all of them. Let's move on to feature selection so we can restrict our feature set to one that is both significant and predictive of the target variable without being so onerous to run calculations on. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are younger homeowners are more likely to default on credit payments? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0$: $\\mu_d$ =  $\\mu_n$<br>\n",
    "$H_1$:  $\\mu_d$ $\\neq$ $\\mu_n$\n",
    "\n",
    "s.t. <br>\n",
    "$\\mu_d$: the average age of clients who default<br>\n",
    "$\\mu_n$: the average age of clients who do not default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test results:\n",
      "\n",
      "t-statistic: 43.517\n",
      "p-value: 0.000\n",
      "\n",
      "Reject the null hypothesis: There is a significant difference in the average ages \n",
      "of clients who default vs. those who don't.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "age_df: DataFrame = application_train[[\"target\", \"days_birth\"]]\n",
    "age_df = age_df.with_columns((pl.col(\"days_birth\") // -365)).rename(\n",
    "    {\"days_birth\": \"age\"}\n",
    ")\n",
    "\n",
    "age_default = age_df.filter(pl.col(\"target\") == 1)[\"age\"]\n",
    "age_no_default = age_df.filter(pl.col(\"target\") == 0)[\"age\"]\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(age_no_default, age_default)\n",
    "\n",
    "print(\"Two-sample t-test results:\\n\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.3f}\\n\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\n",
    "        \"Reject the null hypothesis: There is a significant difference in the average ages \\nof clients who default vs. those who don't.\\n\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Fail to reject the null hypothesis: There is no significant difference in the average ages of clients who \\ndefault vs. those who don't.\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_statistics(y_true, y_predict, beta=3.0, title=\"statistics\"):\n",
    "    \"\"\"Uses actual y and predicted y values to return a dataframe of accuracy, precision, recall, and f-beta values as well as false negative and false posititive rates for a given classifier\n",
    "\n",
    "    Args:\n",
    "        y_true (numpy array or data series): dependent variable values from the dataset\n",
    "        y_predict (_type_): dependent variable values arising from model\n",
    "        beta (float, optional): Beta value to determine weighting between precision and recall in the f-beta score.Defaults to beta value set in global scope of this notebook.\n",
    "        title (str, optional): _description_. Defaults to \"statistics\".\n",
    "\n",
    "    Returns:\n",
    "        model_statistics: pandas dataframe of statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_predict).ravel()\n",
    "\n",
    "    # calculate statistics from confusion matrix\n",
    "    # accuracy: float = accuracy_score(y_true, y_predict)\n",
    "    roc_auc: float = roc_auc_score(y_true, y_predict)\n",
    "    mcc: float = matthews_corrcoef(y_true, y_predict)\n",
    "    f_beta: float = fbeta_score(y_true, y_predict, beta=beta)\n",
    "\n",
    "    precision: float = precision_score(y_true, y_predict, zero_division=0)\n",
    "    recall: float = recall_score(y_true, y_predict)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true,y_predict)\n",
    "    # false_negative_rate: float = fn / (tn + fp + fn + tp)\n",
    "    # false_positive_rate: float = fp / (tn + fp + fn + tp)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        data={\n",
    "            title: [\n",
    "                roc_auc,\n",
    "                mcc,\n",
    "                f_beta,\n",
    "                precision,\n",
    "                recall,\n",
    "                balanced_accuracy\n",
    "                # accuracy,\n",
    "                # false_negative_rate,\n",
    "                # false_positive_rate,\n",
    "            ]\n",
    "        },\n",
    "        index=[\n",
    "            \"roc_auc\",\n",
    "            \"matthews_correlation\",\n",
    "            \"f_beta\",\n",
    "            \"precision\",\n",
    "            \"recall\",\n",
    "            \"balanced_accuracy\"\n",
    "            # \"accuracy\",\n",
    "            # \"false_negative_rate\",\n",
    "            # \"false_positive_rate\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start creating models, it's important to lay out what our north star assessment criteria are for model assessment, and secondarily how we will choose what a high-performing model is. \n",
    "\n",
    "Given that our business problem is designing predictive classifiers of credit clients that are likely to default on payments, and that our training data is highly imbalanced such that our positive class is the minority class at a ratio of 10:1, we will need to be very precise in our metric selection. \n",
    "\n",
    "Therefore each model that we create will be judged on its ROC-AUC score, F1 score, Matthews Correlation Coefficient, Balanced Accuracy, Precision, and Recall. This sounds like a long list of metrics, so our primary determinants will be the ROC-AUC and MCC since ROC-AUC can be less powerful for highly imbalanced datasets, and MCC gives a pretty comprehensive view of a model's performance across all 4 quadrants of the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svdmqv55uwMC"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our metrics set and evaluation method, our workflow for the next section is as follows: \n",
    "1. Create pipelines to impute missing values, scale data, and fit to our classifier of choice\n",
    "2. Calculate model statistics for each classifier\n",
    "\n",
    "\n",
    "Let's setup some initial arrays/dataframes, as well as functions we'll need to use in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cQ9821wAuwMD"
   },
   "outputs": [],
   "source": [
    "application_train = create_formatted_df(\"application_train.csv\")\n",
    "\n",
    "x = (\n",
    "    application_train\n",
    "    # .drop(cs.matches(\"flag_document_\"))\n",
    "    .drop([\"sk_id_curr\", \"target\"]).to_pandas()\n",
    ")\n",
    "y = pl.DataFrame(application_train[\"target\"]).to_pandas()\n",
    "\n",
    "# x_columns = x.columns\n",
    "# y_columns = y.columns\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.3, random_state=0, stratify=y\n",
    ")\n",
    "y_train = np.array(y_train).ravel()\n",
    "\n",
    "numerical_columns = [*x.select_dtypes(exclude=[\"object\", \"category\"]).columns]\n",
    "\n",
    "categorical_columns = [*x.select_dtypes(include=[\"object\", \"category\"]).columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_rTD-Z3yuwMD"
   },
   "outputs": [],
   "source": [
    "def instantiate_numerical_simple_imputer(\n",
    "    trial: Trial, fill_value: int = -1\n",
    ") -> SimpleImputer:\n",
    "    strategy = trial.suggest_categorical(\n",
    "        \"numerical_strategy\", [\"mean\", \"median\", \"most_frequent\", \"constant\"]\n",
    "    )\n",
    "    return SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "\n",
    "\n",
    "def instantiate_categorical_simple_imputer(\n",
    "    trial: Trial, fill_value: str = \"missing\"\n",
    ") -> SimpleImputer:\n",
    "    strategy = trial.suggest_categorical(\n",
    "        \"categorical_strategy\", [\"most_frequent\", \"constant\"]\n",
    "    )\n",
    "    return SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "\n",
    "\n",
    "def instantiate_woe_encoder(trial: Trial) -> WOEEncoder:\n",
    "    params = {\n",
    "        \"sigma\": trial.suggest_float(\"sigma\", 0.001, 5),\n",
    "        \"regularization\": trial.suggest_float(\"regularization\", 0, 5),\n",
    "        \"randomized\": trial.suggest_categorical(\"randomized\", [True, False]),\n",
    "    }\n",
    "    return WOEEncoder(**params)\n",
    "\n",
    "\n",
    "def instantiate_robust_scaler(trial: Trial) -> RobustScaler:\n",
    "    params = {\n",
    "        \"with_centering\": trial.suggest_categorical(\"with_centering\", [True, False]),\n",
    "        \"with_scaling\": trial.suggest_categorical(\"with_scaling\", [True, False]),\n",
    "    }\n",
    "    return RobustScaler(**params)\n",
    "\n",
    "\n",
    "def instantiate_extra_trees(trial: Trial, warm_start=False) -> ExtraTreesClassifier:\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
    "        \"max_features\": trial.suggest_float(\"max_features\", 0, 1),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    return ExtraTreesClassifier(**params, warm_start=warm_start)\n",
    "\n",
    "\n",
    "def instantiate_logistic_regression(trial) -> LogisticRegression:\n",
    "    solver = trial.suggest_categorical(\n",
    "        \"solver\", [\"lbfgs\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]\n",
    "    )\n",
    "    if solver == \"newton-cholesky\":\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "        params = {\n",
    "            \"solver\": solver,\n",
    "            \"penalty\": penalty,\n",
    "            \"C\": trial.suggest_float(\"C\", 0.0, 10.0),\n",
    "        }\n",
    "    elif solver == \"lbfgs\":\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "        params = {\n",
    "            \"solver\": solver,\n",
    "            \"penalty\": penalty,\n",
    "            \"C\": trial.suggest_float(\"C\", 0.0, 10.0),\n",
    "        }\n",
    "    elif solver == \"liblinear\":\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "        params = {\n",
    "            \"solver\": solver,\n",
    "            \"penalty\": penalty,\n",
    "            \"C\": trial.suggest_float(\"C\", 0.0, 10.0),\n",
    "        }\n",
    "    elif solver == \"newton-cg\":\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "        params = {\n",
    "            \"solver\": solver,\n",
    "            \"penalty\": penalty,\n",
    "            \"C\": trial.suggest_float(\"C\", 0.0, 10.0),\n",
    "        }\n",
    "    elif solver == \"sag\":\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "        params = {\n",
    "            \"solver\": solver,\n",
    "            \"penalty\": penalty,\n",
    "            \"C\": trial.suggest_float(\"C\", 0.0, 10.0),\n",
    "        }\n",
    "    elif solver == \"saga\":\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "        params = {\n",
    "            \"solver\": solver,\n",
    "            \"penalty\": penalty,\n",
    "            \"C\": trial.suggest_float(\"C\", 0.0, 10.0),\n",
    "        }\n",
    "    return LogisticRegression(**params)\n",
    "\n",
    "\n",
    "#def instantiate_lgbm_classifier(trial):\n",
    "#    params = {\n",
    "#        \"boosting_type\": trial.suggest_categorical(\n",
    "#            \"boosting_type\", [\"gbdt\", \"dart\", \"rf\"]\n",
    "#        ),\n",
    "#        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 64),\n",
    "#        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 20),\n",
    "#        \"n_estimators\": trial.suggest_int(\"n_estimators\", 35, 150),\n",
    "#        \"class_weight\": \"balanced\",    }\n",
    "#    return LGBMClassifier(**params)\n",
    "\n",
    "\n",
    "\n",
    "def instantiate_lgbm_classifier(trial):\n",
    "    params = {\n",
    "        \"boosting_type\": trial.suggest_categorical(\n",
    "            \"boosting_type\", [\"gbdt\", \"dart\"]#, #\"rf\"]\n",
    "        ),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 64),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 20),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 35, 150),\n",
    "        \"class_weight\": \"balanced\",\n",
    "        # Add the following conditional logic for bagging parameters\n",
    "        #\"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10) if params[\"boosting_type\"] == \"rf\" else 1,  # Ensure bagging_freq > 0 for 'rf'\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.1, 0.99) if params[\"boosting_type\"] == \"rf\" else 1.0,  # Ensure 0 < bagging_fraction < 1 for 'rf'\n",
    "        #\"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.1, 0.99) if params[\"boosting_type\"] != \"rf\" else 1.0,  # Ensure 0 < feature_fraction < 1 for non-'rf'\n",
    "    }\n",
    "    return LGBMClassifier(**params)\n",
    "\n",
    "def instantiate_xgboost(trial):\n",
    "    params = {\n",
    "         \"objective\": trial.suggest_categorical(\n",
    "    \"objective\", [\"binary:hinge\", \"binary:logistic\"]),\n",
    "    \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\", \"gblinear\"]),\n",
    "    \"max_leaves\": trial.suggest_int(\"max_leaves\",1, 10, 10),\n",
    "    \"max_depth\": trial.suggest_int(\"max_depth\",3, 15, 4),\n",
    "    \"grow_policy\": trial.suggest_categorical(\"grow_policy\",[\"depthwise\"]),\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\",50, 100),\n",
    "    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1),\n",
    "}\n",
    "    return XGBClassifier(**params)\n",
    "    \n",
    "\n",
    "\n",
    "def instantiate_random_forest(trial):\n",
    "    params = {\n",
    "        \"criterion\": trial.suggest_categorical(\n",
    "            \"criterion\", [\"gini\", \"entropy\", \"log_loss\"]\n",
    "        ),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 64),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 64),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 35, 150),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "        \"class_weight\": trial.suggest_categorical(\n",
    "            \"class_weight\", [\"balanced\", \"balanced_subsample\"]\n",
    "        ),\n",
    "        \"max_features\": trial.suggest_categorical(\n",
    "            \"max_features\", [\"sqrt\", \"log2\", None]\n",
    "        ),\n",
    "    }\n",
    "    return RandomForestClassifier(**params)\n",
    "\n",
    "\n",
    "#def instantiate_random_forest(trial):\n",
    "\n",
    "    # \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    # \"max_depth\": list(range(1, 11)),\n",
    "    # \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "\n",
    "    #n_estimators = trial.suggest_int(\"n_estimators\", 10, 100)\n",
    "    #max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "    #criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "    \n",
    "   # clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, criterion=criterion)\n",
    "    #return clf\n",
    "\n",
    "\n",
    "def model_selector(clf_string, trial: Trial):\n",
    "    if clf_string == \"logistic_regression\":\n",
    "        model = instantiate_logistic_regression(trial)\n",
    "    elif clf_string == \"random_forest\":\n",
    "        model = instantiate_random_forest(trial)\n",
    "    elif clf_string == \"extra_trees\":\n",
    "        model = instantiate_extra_trees(trial)\n",
    "    elif clf_string == \"lightgbm\":\n",
    "        model = instantiate_lgbm_classifier(trial)\n",
    "    elif clf_string=='xgboost':\n",
    "        model = instantiate_xgboost(trial)\n",
    "\n",
    "    return model\n",
    "\n",
    "def instantiate_numerical_pipeline(trial : Trial) -> Pipeline:\n",
    "  pipeline = Pipeline([\n",
    "    ('imputer', instantiate_numerical_simple_imputer(trial)),\n",
    "    ('scaler', instantiate_robust_scaler(trial))\n",
    "  ])\n",
    "  return pipeline\n",
    "\n",
    "def instantiate_categorical_pipeline(trial : Trial) -> Pipeline:\n",
    "  pipeline = Pipeline([\n",
    "    ('imputer', instantiate_categorical_simple_imputer(trial)),\n",
    "    ('encoder', instantiate_woe_encoder(trial))\n",
    "  ])\n",
    "  return pipeline\n",
    "\n",
    "def instantiate_processor(trial : Trial, numerical_columns : list[str], categorical_columns : list[str]) -> ColumnTransformer:\n",
    "\n",
    "  numerical_pipeline = instantiate_numerical_pipeline(trial)\n",
    "  categorical_pipeline = instantiate_categorical_pipeline(trial)\n",
    "\n",
    "  processor = ColumnTransformer([\n",
    "    ('numerical_pipeline', numerical_pipeline, numerical_columns),\n",
    "    ('categorical_pipeline', categorical_pipeline, categorical_columns)\n",
    "  ])\n",
    "\n",
    "  return processor\n",
    "\n",
    "def instantiate_model(classifier, trial : Trial, numerical_columns : list[str], categorical_columns : list[str]) -> Pipeline:\n",
    "\n",
    "  processor = instantiate_processor(\n",
    "    trial, numerical_columns, categorical_columns\n",
    "  )\n",
    "  \n",
    "  clf = model_selector(classifier,trial)\n",
    "\n",
    "  model = Pipeline([\n",
    "    ('processor', processor),\n",
    "    ('classifier', clf)\n",
    "  ])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKavWOTGuwME"
   },
   "source": [
    "Now let's define the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9Xp9WCzhuwME"
   },
   "outputs": [],
   "source": [
    "def objective(classifier, trial : Trial, X : DataFrame, y : np.ndarray | Series, numerical_columns : Optional[list[str]]=None, categorical_columns : Optional[list[str]]=None, random_state : int=42) -> float:\n",
    "  if numerical_columns is None:\n",
    "    numerical_columns = [\n",
    "      *x.select_dtypes(exclude=['object', 'category']).columns\n",
    "    ]\n",
    "\n",
    "  if categorical_columns is None:\n",
    "    categorical_columns = [\n",
    "      *x.select_dtypes(include=['object', 'category']).columns\n",
    "    ]\n",
    "\n",
    "  model = instantiate_model(classifier,trial, numerical_columns, categorical_columns)\n",
    "\n",
    "  kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "  roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "  scores = cross_val_score(model, X, y, scoring=roc_auc_scorer, cv=kf)\n",
    "\n",
    "  return np.min([np.mean(scores), np.median([scores])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ojKtSwOuuwMF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 22:12:19,503] A new study created in memory with name: optimization\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning:\n",
      "\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning:\n",
      "\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning:\n",
      "\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning:\n",
      "\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning:\n",
      "\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2024-12-17 22:12:36,280] Trial 0 finished with value: 0.7102562883223078 and parameters: {'numerical_strategy': 'median', 'with_centering': False, 'with_scaling': True, 'categorical_strategy': 'constant', 'sigma': 1.1385859183740936, 'regularization': 1.980451188470127, 'randomized': True, 'solver': 'lbfgs', 'penalty': None, 'C': 8.314799368249435}. Best is trial 0 with value: 0.7102562883223078.\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning:\n",
      "\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2024-12-17 22:12:41,007] A new study created in memory with name: optimization\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "[I 2024-12-17 22:17:18,088] Trial 0 finished with value: 0.7365856281718 and parameters: {'numerical_strategy': 'constant', 'with_centering': False, 'with_scaling': False, 'categorical_strategy': 'constant', 'sigma': 1.4768405784688405, 'regularization': 1.692389017161224, 'randomized': False, 'n_estimators': 765, 'max_depth': 10, 'max_features': 0.9529570960893133, 'bootstrap': True}. Best is trial 0 with value: 0.7365856281718.\n",
      "[I 2024-12-17 22:18:42,218] A new study created in memory with name: optimization\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "[I 2024-12-17 22:20:50,565] Trial 0 finished with value: 0.7358913814440303 and parameters: {'numerical_strategy': 'most_frequent', 'with_centering': False, 'with_scaling': False, 'categorical_strategy': 'most_frequent', 'sigma': 1.9408378416579894, 'regularization': 1.2282847334548759, 'randomized': False, 'criterion': 'entropy', 'min_samples_split': 14, 'max_depth': 35, 'n_estimators': 81, 'min_samples_leaf': 16, 'class_weight': 'balanced', 'max_features': 'sqrt'}. Best is trial 0 with value: 0.7358913814440303.\n"
     ]
    }
   ],
   "source": [
    "classifiers: list[str] = [\"logistic_regression\", \"extra_trees\", \"random_forest\"]\n",
    "\n",
    "model_performance = pd.DataFrame()\n",
    "model_specifications = dict()\n",
    "\n",
    "for classifier in classifiers:\n",
    "    study: Study = create_study(study_name=\"optimization\", direction=\"maximize\")\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(classifier, trial, x_train, np.array(y_train).ravel()),\n",
    "        n_trials=1,\n",
    "    )  # n_trials=100 is the original value\n",
    "\n",
    "    model_specifications[classifier]= study.best_params\n",
    "\n",
    "\n",
    "    best_trial: FrozenTrial = study.best_trial\n",
    "    model: Pipeline = instantiate_model(\n",
    "        classifier,\n",
    "        trial=best_trial,\n",
    "        numerical_columns=numerical_columns,\n",
    "        categorical_columns=categorical_columns,\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    model_performance[classifier] = calculate_model_statistics(y_test, predictions)\n",
    "    model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9Ki-PXUG6yL"
   },
   "source": [
    "These models are performing really poorly so far, with both the Matthews Correlation Coefficient and ROC-AUC pointing to the modeling being no better than random guessing then it comes to predicting on defaults. Let's see how well the lightgbm and xgboost classifiers perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 22:21:23,375] A new study created in memory with name: optimization\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 13937, number of negative: 158268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11418\n",
      "[LightGBM] [Info] Number of data points in the train set: 172205, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 13897, number of negative: 158308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11463\n",
      "[LightGBM] [Info] Number of data points in the train set: 172205, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 13946, number of negative: 158260\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11453\n",
      "[LightGBM] [Info] Number of data points in the train set: 172206, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 13807, number of negative: 158399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11467\n",
      "[LightGBM] [Info] Number of data points in the train set: 172206, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 13921, number of negative: 158285\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 172206, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 22:21:33,628] Trial 0 finished with value: 0.7474595659640969 and parameters: {'numerical_strategy': 'most_frequent', 'with_centering': True, 'with_scaling': False, 'categorical_strategy': 'most_frequent', 'sigma': 4.068667775232698, 'regularization': 1.7958582050557452, 'randomized': False, 'boosting_type': 'gbdt', 'num_leaves': 16, 'max_depth': 19, 'n_estimators': 39}. Best is trial 0 with value: 0.7474595659640969.\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 13937, number of negative: 158268\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15337\n",
      "[LightGBM] [Info] Number of data points in the train set: 172205, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 13897, number of negative: 158308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15383\n",
      "[LightGBM] [Info] Number of data points in the train set: 172205, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 13946, number of negative: 158260\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15372\n",
      "[LightGBM] [Info] Number of data points in the train set: 172206, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 13807, number of negative: 158399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15387\n",
      "[LightGBM] [Info] Number of data points in the train set: 172206, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 13921, number of negative: 158285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15377\n",
      "[LightGBM] [Info] Number of data points in the train set: 172206, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 22:21:45,260] Trial 1 finished with value: 0.7435881215375002 and parameters: {'numerical_strategy': 'most_frequent', 'with_centering': True, 'with_scaling': False, 'categorical_strategy': 'most_frequent', 'sigma': 4.0972922880022065, 'regularization': 2.2163619286056373, 'randomized': True, 'boosting_type': 'dart', 'num_leaves': 50, 'max_depth': 18, 'n_estimators': 52}. Best is trial 0 with value: 0.7474595659640969.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17377, number of negative: 197880\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11522\n",
      "[LightGBM] [Info] Number of data points in the train set: 215257, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 22:21:47,713] A new study created in memory with name: optimization\n",
      "/tmp/ipykernel_128348/2503439611.py:131: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/distributions.py:708: UserWarning:\n",
      "\n",
      "The distribution is specified by [1, 10] and step=10, but the range is not divisible by `step`. It will be replaced by [1, 1].\n",
      "\n",
      "/tmp/ipykernel_128348/2503439611.py:132: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:21:49] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:21:59] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:22:08] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:22:18] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:22:28] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "[I 2024-12-17 22:22:35,808] Trial 0 finished with value: 0.4997338893904638 and parameters: {'numerical_strategy': 'most_frequent', 'with_centering': True, 'with_scaling': True, 'categorical_strategy': 'constant', 'sigma': 0.5494747905380349, 'regularization': 4.902307207474827, 'randomized': True, 'objective': 'binary:hinge', 'booster': 'gblinear', 'max_leaves': 1, 'max_depth': 11, 'grow_policy': 'depthwise', 'max_estimators': 56, 'learning_rate': 0.11361447007978284}. Best is trial 0 with value: 0.4997338893904638.\n",
      "/tmp/ipykernel_128348/2503439611.py:131: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/distributions.py:708: UserWarning:\n",
      "\n",
      "The distribution is specified by [1, 10] and step=10, but the range is not divisible by `step`. It will be replaced by [1, 1].\n",
      "\n",
      "/tmp/ipykernel_128348/2503439611.py:132: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:22:37] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:22:54] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:23:09] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:23:25] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:23:41] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n",
      "[I 2024-12-17 22:23:56,105] Trial 1 finished with value: 0.4997461434849935 and parameters: {'numerical_strategy': 'median', 'with_centering': True, 'with_scaling': False, 'categorical_strategy': 'most_frequent', 'sigma': 3.931260111458912, 'regularization': 1.194463522466091, 'randomized': True, 'objective': 'binary:hinge', 'booster': 'gblinear', 'max_leaves': 1, 'max_depth': 3, 'grow_policy': 'depthwise', 'max_estimators': 98, 'learning_rate': 0.0962202320920552}. Best is trial 1 with value: 0.4997461434849935.\n",
      "/tmp/ipykernel_128348/2503439611.py:131: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/distributions.py:708: UserWarning:\n",
      "\n",
      "The distribution is specified by [1, 10] and step=10, but the range is not divisible by `step`. It will be replaced by [1, 1].\n",
      "\n",
      "/tmp/ipykernel_128348/2503439611.py:132: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:23:58] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"grow_policy\", \"max_depth\", \"max_leaves\" } are not used.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers: list[str] = [\"lightgbm\",\n",
    "                          \"xgboost\"]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    study = create_study(study_name=\"optimization\", direction=\"maximize\")\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(classifier, trial, x_train, np.array(y_train).ravel()),\n",
    "        n_trials=2,\n",
    "    )  # n_trials=100 is the original value\n",
    "\n",
    "    model_specifications[classifier]= study.best_params\n",
    "\n",
    "\n",
    "    best_trial: FrozenTrial = study.best_trial\n",
    "    model: Pipeline = instantiate_model(\n",
    "        classifier,\n",
    "        trial=best_trial,\n",
    "        numerical_columns=numerical_columns,\n",
    "        categorical_columns=categorical_columns,\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    model_performance[classifier] = calculate_model_statistics(y_test, predictions)\n",
    "    model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logistic_regression</th>\n",
       "      <th>extra_trees</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>xgboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.500772</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.614881</td>\n",
       "      <td>0.683870</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matthews_correlation</th>\n",
       "      <td>0.023344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208138</td>\n",
       "      <td>0.212613</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_beta</th>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304293</td>\n",
       "      <td>0.511215</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250323</td>\n",
       "      <td>0.162757</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311762</td>\n",
       "      <td>0.670784</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <td>0.500772</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.614881</td>\n",
       "      <td>0.683870</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      logistic_regression  extra_trees  random_forest  \\\n",
       "roc_auc                          0.500772          0.5       0.614881   \n",
       "matthews_correlation             0.023344          0.0       0.208138   \n",
       "f_beta                           0.001939          0.0       0.304293   \n",
       "precision                        0.433333          0.0       0.250323   \n",
       "recall                           0.001745          0.0       0.311762   \n",
       "balanced_accuracy                0.500772          0.5       0.614881   \n",
       "\n",
       "                      lightgbm  xgboost  \n",
       "roc_auc               0.683870      0.5  \n",
       "matthews_correlation  0.212613      0.0  \n",
       "f_beta                0.511215      0.0  \n",
       "precision             0.162757      0.0  \n",
       "recall                0.670784      0.0  \n",
       "balanced_accuracy     0.683870      0.5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the above code with successive halving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def instantiate_extra_trees(trial: Trial, warm_start=False) -> ExtraTreesClassifier:\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
    "        \"max_features\": trial.suggest_float(\"max_features\", 0, 1),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    return ExtraTreesClassifier(**params, warm_start=warm_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pandas import DataFrame\n",
    "from optuna import Trial\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def objective(classifier_string: str, trial : Trial, X : DataFrame, y : DataFrame, seed : int=42) -> Optional[float]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, shuffle=True, random_state=seed\n",
    "    )\n",
    "    #model = instantiate_model(classifier=classifier_string,trial=trial, numerical_columns=numerical_columns, categorical_columns=categorical_columns)\n",
    "    model = model_selector(classifier_string, trial) #instantiate_extra_trees(trial, warm_start=True)\n",
    "    n_estimators = model.get_params().get('n_estimators')\n",
    "    min_estimators = 45\n",
    "    \n",
    "    for num_estimators in range(min_estimators, n_estimators + 1):\n",
    "        model.set_params(n_estimators=num_estimators)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        trial.report(score, num_estimators)\n",
    "    \n",
    "        if trial.should_prune():\n",
    "            pass#raise TrialPruned()\n",
    "\n",
    "    kfold = KFold(shuffle=True, random_state=seed)\n",
    "    roc_auc = make_scorer(roc_auc_score, needs_proba=True)\n",
    "    scores = cross_val_score(model, X, y, cv=kfold, scoring=roc_auc)\n",
    "    \n",
    "    return np.min([np.mean(scores), np.median(scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 22:31:14,834] A new study created in memory with name: no-name-14823518-2141-453e-a685-51890930e2d7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19876, number of negative: 226132\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12259\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 19888, number of negative: 226121\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12240\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 19743, number of negative: 226266\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12250\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 19921, number of negative: 226088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12248\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 19872, number of negative: 226137\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12265\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 22:31:28,271] Trial 0 finished with value: 0.7101552933890023 and parameters: {'boosting_type': 'dart', 'num_leaves': 48, 'max_depth': 12, 'n_estimators': 53}. Best is trial 0 with value: 0.7101552933890023.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029941 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 18634, number of negative: 211999\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12208\n",
      "[LightGBM] [Info] Number of data points in the train set: 230633, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19876, number of negative: 226132\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12259\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 19888, number of negative: 226121\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12240\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 19743, number of negative: 226266\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12250\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 19921, number of negative: 226088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12248\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 19872, number of negative: 226137\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12265\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 22:33:18,123] Trial 1 finished with value: 0.7145099764014973 and parameters: {'boosting_type': 'gbdt', 'num_leaves': 56, 'max_depth': 12, 'n_estimators': 117}. Best is trial 1 with value: 0.7145099764014973.\n"
     ]
    }
   ],
   "source": [
    "numerical_train = application_train.clone()\n",
    "encoder_mapping_key = dict()\n",
    "for col in numerical_train.columns:\n",
    "    try:\n",
    "        key: dict[str, int] = create_encoder_mapping(numerical_train, col)\n",
    "        numerical_train = encode_feature(numerical_train, col, key)\n",
    "        encoder_mapping_key[col] = key\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "x = numerical_train.drop(\"target\")\n",
    "y = numerical_train[\"target\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=True)\n",
    "\n",
    "study = create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=SuccessiveHalvingPruner(reduction_factor=2),\n",
    "    sampler=RandomSampler(seed=42),\n",
    ")\n",
    "study.optimize(\n",
    "    lambda trial: objective(\"lightgbm\", trial, x, y), n_trials=2\n",
    ")  # 30 trials is 12m 35.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'num_leaves': 56,\n",
       " 'max_depth': 12,\n",
       " 'n_estimators': 117}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_specifications['lgbm_with_nulls']= study.best_params\n",
    "\n",
    "\n",
    "lbgm_no_nulls=LGBMClassifier(**study.best_params)\n",
    "lbgm_no_nulls.fit(x_train, y_train)\n",
    "predictions = lbgm_no_nulls.predict(x_test)\n",
    "model_performance[\"lgbm_with_nulls\"] = calculate_model_statistics(y_test, predictions)\n",
    "model_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 22:45:53,107] A new study created in memory with name: no-name-285246cd-a2c1-4856-b0e9-1be569cf21d8\n",
      "/tmp/ipykernel_128348/1745918092.py:131: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/distributions.py:708: UserWarning:\n",
      "\n",
      "The distribution is specified by [1, 10] and step=10, but the range is not divisible by `step`. It will be replaced by [1, 1].\n",
      "\n",
      "/tmp/ipykernel_128348/1745918092.py:132: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "[I 2024-12-17 22:46:25,140] Trial 0 finished with value: 0.5 and parameters: {'objective': 'binary:logistic', 'booster': 'gbtree', 'max_leaves': 1, 'max_depth': 3, 'grow_policy': 'depthwise', 'n_estimators': 52, 'learning_rate': 0.8675143843171859}. Best is trial 0 with value: 0.5.\n",
      "/tmp/ipykernel_128348/1745918092.py:131: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/distributions.py:708: UserWarning:\n",
      "\n",
      "The distribution is specified by [1, 10] and step=10, but the range is not divisible by `step`. It will be replaced by [1, 1].\n",
      "\n",
      "/tmp/ipykernel_128348/1745918092.py:132: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "[I 2024-12-17 22:55:10,194] Trial 1 finished with value: 0.5 and parameters: {'objective': 'binary:logistic', 'booster': 'dart', 'max_leaves': 1, 'max_depth': 3, 'grow_policy': 'depthwise', 'n_estimators': 59, 'learning_rate': 0.1915704647548995}. Best is trial 0 with value: 0.5.\n"
     ]
    }
   ],
   "source": [
    "study = create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=SuccessiveHalvingPruner(reduction_factor=2),\n",
    "    sampler=RandomSampler(seed=42),\n",
    ")\n",
    "study.optimize(\n",
    "    lambda trial: objective(\"xgboost\", trial, x, y), n_trials=2\n",
    ")\n",
    "\n",
    "xgb_no_nulls=XGBClassifier(**study.best_params)\n",
    "xgb_no_nulls.fit(x_train, y_train)\n",
    "predictions = xgb_no_nulls.predict(x_test)\n",
    "model_performance[\"xgboost_with_nulls\"] = calculate_model_statistics(y_test, predictions)\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE Oversampling \n",
    "One way to strengthen our model performance could be to oversample/undersample on our dataset to rebalance the proportion of positive and negative target classes, and then train our classifier on that. We'll use the method put forth in the original SMOTE paper and then retrain on the models shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "oversampling = SMOTE(sampling_strategy=0.1)\n",
    "undersampling = RandomUnderSampler(sampling_strategy=0.5)\n",
    "\n",
    "steps = [(\"oversample\", oversampling), (\"undersample\", undersampling)]\n",
    "pipeline = ImbPipeline(steps=steps)\n",
    "smote_x, smote_y = pipeline.fit_resample(x, y)\n",
    "\n",
    "pca = PCA(n_components=smote_x.shape[1])\n",
    "smote_x = pca.fit_transform(smote_x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    smote_x, smote_y, stratify=smote_y, random_state=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our smote dataset, we can fit the models. However our previous model specifications aren't as helpful since the dataset is new, and models may perform better on different hyperparameters. Let's create another study and run it again! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 23:13:37,310] A new study created in memory with name: no-name-f274b13d-68ed-4537-8fcf-2a06fcac1e27\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "[I 2024-12-17 23:13:49,852] Trial 0 finished with value: 0.7101552933890023 and parameters: {'boosting_type': 'dart', 'num_leaves': 48, 'max_depth': 12, 'n_estimators': 53}. Best is trial 0 with value: 0.7101552933890023.\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "[I 2024-12-17 23:15:34,958] Trial 1 finished with value: 0.7145099764014973 and parameters: {'boosting_type': 'gbdt', 'num_leaves': 56, 'max_depth': 12, 'n_estimators': 117}. Best is trial 1 with value: 0.7145099764014973.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logistic_regression</th>\n",
       "      <th>extra_trees</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>xgb_with_nulls</th>\n",
       "      <th>xgboost_with_nulls</th>\n",
       "      <th>SMOTE_lgbm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.500772</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.614881</td>\n",
       "      <td>0.503748</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.628378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matthews_correlation</th>\n",
       "      <td>0.023344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208138</td>\n",
       "      <td>0.053637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_beta</th>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304293</td>\n",
       "      <td>0.009254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250323</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311762</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.359276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <td>0.500772</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.614881</td>\n",
       "      <td>0.503748</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.628378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      logistic_regression  extra_trees  random_forest  \\\n",
       "roc_auc                          0.500772          0.5       0.614881   \n",
       "matthews_correlation             0.023344          0.0       0.208138   \n",
       "f_beta                           0.001939          0.0       0.304293   \n",
       "precision                        0.433333          0.0       0.250323   \n",
       "recall                           0.001745          0.0       0.311762   \n",
       "balanced_accuracy                0.500772          0.5       0.614881   \n",
       "\n",
       "                      lightgbm  xgboost  xgb_with_nulls  xgboost_with_nulls  \\\n",
       "roc_auc               0.503748      0.5             0.5                 0.5   \n",
       "matthews_correlation  0.053637      0.0             0.0                 0.0   \n",
       "f_beta                0.009254      0.0             0.0                 0.0   \n",
       "precision             0.464286      0.0             0.0                 0.0   \n",
       "recall                0.008345      0.0             0.0                 0.0   \n",
       "balanced_accuracy     0.503748      0.5             0.5                 0.5   \n",
       "\n",
       "                      SMOTE_lgbm  \n",
       "roc_auc                 0.628378  \n",
       "matthews_correlation    0.309718  \n",
       "f_beta                  0.375642  \n",
       "precision               0.636660  \n",
       "recall                  0.359276  \n",
       "balanced_accuracy       0.628378  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=SuccessiveHalvingPruner(reduction_factor=2),\n",
    "    sampler=RandomSampler(seed=42),\n",
    ")\n",
    "study.optimize(lambda trial: objective(\"lightgbm\", trial, x, y), n_trials=2)\n",
    "\n",
    "\n",
    "model_specifications[\"SMOTE_lgbm\"] = study.best_params\n",
    "\n",
    "smote_lgbm = LGBMClassifier(**study.best_params)\n",
    "smote_lgbm.fit(x_train, y_train)\n",
    "predictions = smote_lgbm.predict(x_test)\n",
    "model_performance[\"SMOTE_lgbm\"] = calculate_model_statistics(y_test, predictions)\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 23:15:35,747] A new study created in memory with name: no-name-c01bd11f-aa94-4fff-a5d6-651b669d953d\n",
      "/tmp/ipykernel_128348/1745918092.py:131: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/distributions.py:708: UserWarning:\n",
      "\n",
      "The distribution is specified by [1, 10] and step=10, but the range is not divisible by `step`. It will be replaced by [1, 1].\n",
      "\n",
      "/tmp/ipykernel_128348/1745918092.py:132: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "[I 2024-12-17 23:15:54,091] Trial 0 finished with value: 0.5 and parameters: {'objective': 'binary:logistic', 'booster': 'gbtree', 'max_leaves': 1, 'max_depth': 3, 'grow_policy': 'depthwise', 'n_estimators': 52, 'learning_rate': 0.8675143843171859}. Best is trial 0 with value: 0.5.\n",
      "/tmp/ipykernel_128348/1745918092.py:131: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/distributions.py:708: UserWarning:\n",
      "\n",
      "The distribution is specified by [1, 10] and step=10, but the range is not divisible by `step`. It will be replaced by [1, 1].\n",
      "\n",
      "/tmp/ipykernel_128348/1745918092.py:132: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning:\n",
      "\n",
      "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "\n",
      "[I 2024-12-17 23:32:08,270] Trial 1 finished with value: 0.5 and parameters: {'objective': 'binary:logistic', 'booster': 'dart', 'max_leaves': 1, 'max_depth': 3, 'grow_policy': 'depthwise', 'n_estimators': 59, 'learning_rate': 0.1915704647548995}. Best is trial 0 with value: 0.5.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logistic_regression</th>\n",
       "      <th>extra_trees</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>xgb_with_nulls</th>\n",
       "      <th>xgboost_with_nulls</th>\n",
       "      <th>SMOTE_lgbm</th>\n",
       "      <th>SMOTE_xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.500772</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.614881</td>\n",
       "      <td>0.503748</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.628378</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matthews_correlation</th>\n",
       "      <td>0.023344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208138</td>\n",
       "      <td>0.053637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_beta</th>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304293</td>\n",
       "      <td>0.009254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375642</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250323</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311762</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.359276</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <td>0.500772</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.614881</td>\n",
       "      <td>0.503748</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.628378</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      logistic_regression  extra_trees  random_forest  \\\n",
       "roc_auc                          0.500772          0.5       0.614881   \n",
       "matthews_correlation             0.023344          0.0       0.208138   \n",
       "f_beta                           0.001939          0.0       0.304293   \n",
       "precision                        0.433333          0.0       0.250323   \n",
       "recall                           0.001745          0.0       0.311762   \n",
       "balanced_accuracy                0.500772          0.5       0.614881   \n",
       "\n",
       "                      lightgbm  xgboost  xgb_with_nulls  xgboost_with_nulls  \\\n",
       "roc_auc               0.503748      0.5             0.5                 0.5   \n",
       "matthews_correlation  0.053637      0.0             0.0                 0.0   \n",
       "f_beta                0.009254      0.0             0.0                 0.0   \n",
       "precision             0.464286      0.0             0.0                 0.0   \n",
       "recall                0.008345      0.0             0.0                 0.0   \n",
       "balanced_accuracy     0.503748      0.5             0.5                 0.5   \n",
       "\n",
       "                      SMOTE_lgbm  SMOTE_xgb  \n",
       "roc_auc                 0.628378        0.5  \n",
       "matthews_correlation    0.309718        0.0  \n",
       "f_beta                  0.375642        0.0  \n",
       "precision               0.636660        0.0  \n",
       "recall                  0.359276        0.0  \n",
       "balanced_accuracy       0.628378        0.5  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=SuccessiveHalvingPruner(reduction_factor=2),\n",
    "    sampler=RandomSampler(seed=42),\n",
    ")\n",
    "study.optimize(lambda trial: objective(\"xgboost\", trial, x, y), n_trials=2)\n",
    "\n",
    "smote_xgb = XGBClassifier(**study.best_params)\n",
    "smote_xgb.fit(x_train, y_train)\n",
    "predictions = smote_xgb.predict(x_test)\n",
    "model_performance[\"SMOTE_xgb\"] = calculate_model_statistics(y_test, predictions)\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01sF7EPMG6yL"
   },
   "source": [
    "## Code for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMote, but with previous lgbm no nulls model specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_lgbm = LGBMClassifier(**model_specifications[\"lgbm_with_nulls\"], verbose=-1)\n",
    "smote_lgbm.fit(x_train, y_train)\n",
    "\n",
    "y_predict = smote_lgbm.predict(x_test)\n",
    "\n",
    "smote_performance: tuple[DataFrame] = calculate_model_statistics(y_test, y_predict)\n",
    "model_performance[\"SMOTE_lgbm\"] = smote_performance\n",
    "\n",
    "\n",
    "pd.concat(\n",
    "    [model_performance[\"lightgbm\"], smote_performance],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_sgd_classifier(trial, random_state=0):\n",
    "    params = {\n",
    "        \"loss\": trial.suggest_categorical(\n",
    "            \"loss\",\n",
    "            [\n",
    "                \"hinge\",\n",
    "                \"log_loss\",\n",
    "                \"modified_huber\",\n",
    "                \"squared_hinge\",\n",
    "                \"perceptron\",\n",
    "                \"squared_error\",\n",
    "                \"huber\",\n",
    "                \"epsilon_insensitive\",\n",
    "                \"squared_epsilon_insensitive\",\n",
    "            ],\n",
    "        ), \"penalty\": trial.suggest_categorical('penalty',['l2','l1', 'elasticnet', None]),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\",0.0, 1000),\n",
    "        \"l1_ratio\": trial.suggest_float(\"l1_ratio\",0.0, 1.0)}\n",
    "    return SGDClassifier(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the features manually to start\n",
    "encoder_mapping_key = dict()\n",
    "for col in application_train.columns:\n",
    "    try:\n",
    "        key: dict[str, int] = create_encoder_mapping(application_train, col)\n",
    "        numerical_train = encode_feature(application_train, col, key)\n",
    "        encoder_mapping_key[col] = key\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "x = numerical_train.drop(\"target\")\n",
    "y = numerical_train[\"target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "_XN9JpguG6yL"
   },
   "outputs": [],
   "source": [
    "from optuna import Trial\n",
    "from sklego.preprocessing import ColumnSelector\n",
    "\n",
    "\n",
    "def choose_columns(trial : Trial, columns : list[str]) -> list[str]:\n",
    "  choose = lambda column: trial.suggest_categorical(column, [True, False])\n",
    "  choices = [*filter(choose, columns)]\n",
    "  return choices\n",
    "\n",
    "\n",
    "def instantiate_column_selector(trial : Trial, columns : list[str]) -> ColumnSelector:\n",
    "  choose = lambda column: trial.suggest_categorical(column, [True, False])\n",
    "  choices = [*filter(choose, columns)]\n",
    "  selector = ColumnSelector(choices)\n",
    "  return selector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "qcjkI0cKG6yL"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "\n",
    "Classifier = (\n",
    "    RandomForestClassifier\n",
    "    | ExtraTreesClassifier\n",
    "    | SVC\n",
    "    | LogisticRegression\n",
    "    | KNeighborsClassifier\n",
    ")\n",
    "\n",
    "\n",
    "def instantiate_learner(trial: Trial) -> Classifier:\n",
    "    algorithm = trial.suggest_categorical(\n",
    "        \"algorithm\", [\"logistic\", \"forest\", \"extra_forest\", \"lgbm\"]\n",
    "    )\n",
    "    if algorithm == \"logistic\":\n",
    "        model = instantiate_logistic_regression(trial)\n",
    "    elif algorithm == \"forest\":\n",
    "        model = instantiate_random_forest(trial)\n",
    "    elif algorithm == \"extra_forest\":\n",
    "        model = instantiate_extra_trees(trial)\n",
    "    elif algorithm == \"lgbm\":\n",
    "        model = instantiate_lgbm_classifier(trial)\n",
    "    # elif algorithm=='knn':\n",
    "    #  model = instantiate_knn(trial)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def instantiate_scaler(trial):\n",
    "    scaler_type = trial.suggest_categorical(\n",
    "        \"scaler_type\", [\"standard\", \"minmax\", \"robust\"]\n",
    "    )\n",
    "\n",
    "    if scaler_type == \"standard\":\n",
    "        params = {\n",
    "            \"with_mean\": trial.suggest_categorical(\"with_mean\", [True, False]),\n",
    "            \"with_std\": trial.suggest_categorical(\"with_std\", [True, False]),\n",
    "        }\n",
    "        scaler = StandardScaler(**params)\n",
    "\n",
    "    elif scaler_type == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    elif scaler_type == \"robust\":\n",
    "        params = {\n",
    "            \"with_centering\": trial.suggest_categorical(\n",
    "                \"with_centering\", [True, False]\n",
    "            ),\n",
    "            \"with_scaling\": trial.suggest_categorical(\"with_std\", [True, False]),\n",
    "        }\n",
    "        scaler = RobustScaler(**params)\n",
    "\n",
    "    return scaler\n",
    "\n",
    "#def instantiate_encoder(trial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial : Trial, x : DataFrame, y : DataFrame, seed : int=42):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, shuffle=True, random_state=seed\n",
    "    )\n",
    "    \n",
    "    model = instantiate_extra_trees(trial, warm_start=True)\n",
    "    n_estimators = model.get_params().get('n_estimators')\n",
    "    min_estimators = 100\n",
    "    \n",
    "    for num_estimators in range(min_estimators, n_estimators + 1):\n",
    "        model.set_params(n_estimators=num_estimators)\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        score = roc_auc_score(y_test, model.predict_proba(x_test)[:, 1])\n",
    "        trial.report(score, num_estimators)\n",
    "    \n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    kfold = KFold(shuffle=True, random_state=seed)\n",
    "    roc_auc = make_scorer(roc_auc_score, needs_proba=True)\n",
    "    scores = cross_val_score(model, x, y, cv=kfold, scoring=roc_auc)\n",
    "    \n",
    "    return np.min([np.mean(scores), np.median(scores)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDBzuzKHG6yL",
    "outputId": "82eab09e-5979-4580-a1ee-e908a6c3d428"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 15:11:00,130] A new study created in memory with name: optimization\n",
      "[W 2024-12-08 15:11:00,514] Trial 0 failed with parameters: {'n_estimators': 807, 'max_depth': 7, 'max_features': 0.21296783552104626, 'bootstrap': False} because of the following error: ValueError(\"could not convert string to float: 'cash loans'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_187908/244823179.py\", line 52, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, x_train, np.array(y_train).ravel()), n_trials=5) #n_trials=100 is the original value\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_187908/1609094541.py\", line 12, in objective\n",
      "    model.fit(x_train, y_train)\n",
      "  File \"/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 363, in fit\n",
      "    X, y = self._validate_data(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/base.py\", line 650, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n",
      "    X = check_array(\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1012, in check_array\n",
      "    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 745, in _asarray_with_order\n",
      "    array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lemuelrobinson/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/pandas/core/generic.py\", line 2153, in __array__\n",
      "    arr = np.asarray(values, dtype=dtype)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: could not convert string to float: 'cash loans'\n",
      "[W 2024-12-08 15:11:00,516] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'cash loans'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_187908/244823179.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_study\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'optimization'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#n_trials=100 is the original value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    115\u001b[0m                         )\n\u001b[1;32m    116\u001b[0m                     )\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_optimize_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;31m# Please refer to the following PR for further details:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# https://github.com/optuna/optuna/pull/325.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_187908/244823179.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(trial)\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#n_trials=100 is the original value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_187908/1609094541.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(trial, x, y, seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmin_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum_estimators\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1469\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1470\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"estimator\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         raise ValueError(\n\u001b[1;32m   1298\u001b[0m             \u001b[0;34mf\"{estimator_name} requires y to be passed, but the target y is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1302\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                         )\n\u001b[1;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m                 raise ValueError(\n\u001b[1;32m   1015\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 ) from complex_warning\n",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Credit_Capstone/.venv/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     def __array__(\n\u001b[1;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'cash loans'"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def instantiate_numerical_pipeline(trial : Trial) -> Pipeline:\n",
    "  pipeline = Pipeline([\n",
    "    ('imputer', instantiate_numerical_simple_imputer(trial)),\n",
    "    ('scaler', instantiate_scaler(trial))\n",
    "  ])\n",
    "  return pipeline\n",
    "\n",
    "def instantiate_categorical_pipeline(trial : Trial) -> Pipeline:\n",
    "  pipeline = Pipeline([\n",
    "    ('imputer', instantiate_categorical_simple_imputer(trial)),\n",
    "    ('encoder', instantiate_woe_encoder(trial))#instantiate_encoder(trial))\n",
    "  ])\n",
    "  return pipeline\n",
    "\n",
    "def instantiate_processor(trial : Trial, numerical_columns : list[str], categorical_columns : list[str]) -> ColumnTransformer:\n",
    "\n",
    "  numerical_pipeline = instantiate_numerical_pipeline(trial)\n",
    "  categorical_pipeline = instantiate_categorical_pipeline(trial)\n",
    "\n",
    "  selected_numerical_columns = choose_columns(trial,numerical_columns)\n",
    "  selected_categorical_columns = choose_columns(trial,categorical_columns)\n",
    "\n",
    "  processor = ColumnTransformer([\n",
    "    ('numerical_pipeline', numerical_pipeline, selected_numerical_columns),\n",
    "    ('categorical_pipeline', categorical_pipeline, selected_categorical_columns)\n",
    "  ])\n",
    "\n",
    "  return processor\n",
    "\n",
    "def instantiate_model(trial : Trial, numerical_columns : list[str], categorical_columns : list[str]) -> Pipeline:\n",
    "\n",
    "  processor = instantiate_processor(\n",
    "    trial, numerical_columns, categorical_columns\n",
    "  )\n",
    "\n",
    "  learner = instantiate_learner(trial)\n",
    "\n",
    "  model = Pipeline([\n",
    "    ('processor', processor),\n",
    "    ('model', learner)\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "from optuna import create_study\n",
    "\n",
    "study = create_study(study_name='optimization', direction='maximize')\n",
    "\n",
    "study.optimize(lambda trial: objective(trial, x_train, np.array(y_train).ravel()), n_trials=5) #n_trials=100 is the original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7e9EIeUG6yL"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcwNjbHluwMG"
   },
   "outputs": [],
   "source": [
    "study = create_study(study_name='optimization', direction='maximize')\n",
    "\n",
    "study.optimize(lambda trial: objective(classifier='random_forest',trial=trial, X=x_train, y=np.array(y_train).ravel()), n_trials=1) #n_trials=100 is the original value\n",
    "\n",
    "best_trial: FrozenTrial = study.best_trial\n",
    "model: Pipeline = instantiate_model('random_forest',trial=best_trial, numerical_columns=numerical_columns, categorical_columns=categorical_columns)\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "calculate_model_statistics(y_true=y_test, y_predict=predictions)\n",
    "\n",
    "#study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "\n",
    "from optuna import create_study\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "from optuna.samplers import RandomSampler\n",
    "\n",
    "study = create_study(\n",
    "  direction=\"maximize\",\n",
    "  pruner=SuccessiveHalvingPruner(reduction_factor=2),\n",
    "  sampler=RandomSampler(seed=42) \n",
    ")\n",
    "study.optimize(lambda trial: objective(trial, x, y), n_trials=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gZV-cGpuwMG"
   },
   "outputs": [],
   "source": [
    "def iterative_bayesian_search(classifier, x_train, y_train, parameter_grid):\n",
    "    final_parameters = dict()\n",
    "    # define iteration dictionary\n",
    "\n",
    "    for key, value in parameter_grid.items():\n",
    "\n",
    "        #final_parameters[key]=value\n",
    "        iteration_grid = {key: value}\n",
    "        iteration_grid = {**final_parameters, **iteration_grid}\n",
    "\n",
    "        # do bayesian search\n",
    "        model_parameters = skopt_bayesian_search(\n",
    "            classifier, x_train, y_train, iteration_grid\n",
    "        )\n",
    "\n",
    "        # isolate iteration_grid parameter\n",
    "        final_parameters[key] = model_parameters[key]\n",
    "\n",
    "    return final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAb4tHVSuwMH"
   },
   "outputs": [],
   "source": [
    "def restrict_x_columns(x_train,x_test, columns):\n",
    "    x_train=x_train[:,columns]\n",
    "    x_test=x_test[:,columns]\n",
    "    return x_train,x_test\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hi5NJpxuwMH"
   },
   "source": [
    "Let's first implement bayesian optimization on a random forest model to see how well the data performs there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d4dw4rEuwMH"
   },
   "outputs": [],
   "source": [
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "\n",
    "model = TPOTClassifier(\n",
    "    generations=3,\n",
    "    population_size=50,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    verbosity=2,\n",
    "    random_state=15,\n",
    "    n_jobs=-1,\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "\n",
    "best_pipeline = model.fitted_pipeline_\n",
    "tpot_classifier = clone(best_pipeline)\n",
    "tpot_classifier_params = tpot_classifier.steps[-1][1].get_params()\n",
    "tpot_classifier_name = str(type(best_pipeline.steps[-1][1])).split(\".\")[-1][:-2]\n",
    "\n",
    "# Fit the new classifier on the training data\n",
    "tpot_classifier.fit(training_x, training_y)\n",
    "predictions = tpot_classifier.predict(validation_x)\n",
    "tpot_accuracy = tpot_classifier.score(validation_x, validation_y)\n",
    "print(f\"\\n{tpot_classifier_name} accuracy is {tpot_accuracy:.3f}\")\n",
    "\n",
    "model_stats_df = calculate_model_statistics(\n",
    "    y_true=validation_y, y_predict=predictions, title=tpot_classifier_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bRnZ1fJuwMH"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"boosting_type\": [\"gbdt\"],\n",
    "    \"num_leaves\": int_range(2, 15),\n",
    "    \"max_depth\": int_range(1, 15),\n",
    "    \"n_estimators\": np.linspace(50, 150, 10, dtype=int),#[50],\n",
    "    \"n_estimators\": np.linspace(50, 300, 10, dtype=int),\n",
    "    \"reg_alpha\": np.linspace(0, 1, 10),\n",
    "    \"reg_lambda\": np.linspace(0, 1, 10),\n",
    "    \"subsample\": np.linspace(0.1, 1, 20),\n",
    "}\n",
    "\n",
    "n_iter = 200\n",
    "classifier = LGBMClassifier(verbose=-1)\n",
    "metric = \"precision\"\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    classifier, param_distributions=params, n_iter=n_iter, scoring=metric\n",
    ")\n",
    "random_search.fit(x_train, y_train)\n",
    "#print(random_search.best_score_, random_search.best_params_)\n",
    "lgbm_params=random_search.best_params_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = random_search.predict(x_test)\n",
    "\n",
    "lgbm_metric: float = random_search.score(x_test, y_test)\n",
    "print(f\"LGBM classifier metric is {lgbm_metric:.3f}\")\n",
    "calculate_model_statistics(y_true=y_test, y_predict=predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lgbm_classifier: LGBMClassifier = LGBMClassifier(**lgbm_params).fit(\n",
    "    x_train, y_train\n",
    ")\n",
    "#predictions = lgbm_classifier.predict(x_test)\n",
    "\n",
    "#lgbm_accuracy: float = lgbm_classifier.score(x_test, y_test)\n",
    "#print(f\"LGBM classifier accuracy is {lgbm_accuracy:.3f}\")\n",
    "calculate_model_statistics(y_true=y_test, y_predict=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvIeAj1guwMI"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "\n",
    "# Declare xgboost search space for Hyperopt\n",
    "xgboost_space = {\n",
    "    \"max_depth\": hp.choice(\"x_max_depth\", [2, 3, 4, 5, 6]),\n",
    "    \"min_child_weight\": hp.choice(\n",
    "        \"x_min_child_weight\", np.round(np.arange(0.0, 0.2, 0.01), 5)\n",
    "    ),\n",
    "    \"learning_rate\": hp.choice(\n",
    "        \"x_learning_rate\", np.round(np.arange(0.005, 0.3, 0.01), 5)\n",
    "    ),\n",
    "    \"subsample\": hp.choice(\"x_subsample\", np.round(np.arange(0.1, 1.0, 0.05), 5)),\n",
    "    \"colsample_bylevel\": hp.choice(\n",
    "        \"x_colsample_bylevel\", np.round(np.arange(0.1, 1.0, 0.05), 5)\n",
    "    ),\n",
    "    \"colsample_bytree\": hp.choice(\n",
    "        \"x_colsample_bytree\", np.round(np.arange(0.1, 1.0, 0.05), 5)\n",
    "    ),\n",
    "    \"n_estimators\": hp.choice(\"x_n_estimators\", np.arange(25, 100, 5)),\n",
    "}\n",
    "\n",
    "best_score = 1.0\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "\n",
    "    global best_score\n",
    "    model = XGBClassifier(**space, n_jobs=-1)\n",
    "    kfold = KFold(n_splits=3, random_state=0, shuffle=True)\n",
    "    score = -cross_val_score(\n",
    "        model, x_train, y_train, cv=kfold, scoring=\"neg_log_loss\", verbose=False\n",
    "    ).mean()\n",
    "\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "xgb_best_params = fmin(\n",
    "    objective, space=xgboost_space, algo=tpe.suggest, max_evals=200, trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Hyperopt search took %.2f seconds for 200 candidates\" % ((time.time() - start)))\n",
    "print(\"Best score: %.2f \" % (-best_score))\n",
    "print(\"Best space: \", xgb_best_params)\n",
    "xgb = XGBClassifier(**xgb_best_params)\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "predictions = xgb.predict(x_test)\n",
    "\n",
    "xgb_metric: float = random_search.score(x_test, y_test)\n",
    "print(f\"XGB classifier metric is {xgb_metric:.3f}\")\n",
    "calculate_model_statistics(y_true=y_test, y_predict=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPqAmTHLuwMI"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": np.linspace(35, 150, 50, dtype=int),\n",
    "    \"learning_rate\": np.linspace(0.01, 0.3, 12),\n",
    "}\n",
    "\n",
    "adaboost_params = skopt_bayesian_search(\n",
    "    AdaBoostClassifier(algorithm=\"SAMME\"), x_train, y_train, params\n",
    ")\n",
    "adaboost_classifier: AdaBoostClassifier = AdaBoostClassifier(\n",
    "    algorithm=\"SAMME\", **adaboost_params\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "predictions = adaboost_classifier.predict(x_test)\n",
    "adaboost_accuracy: float = adaboost_classifier.score(x_test, y_test)\n",
    "print(f\"AdaBoost classifier accuracy is {adaboost_accuracy:.3f}\")\n",
    "model_stats_df['AdaboostClassifier'] = calculate_model_statistics(y_true=y_test, y_predict=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m1PpUU5uwMI"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yecM8wo2uwMJ"
   },
   "source": [
    "### Null value tolerant machine learning models\n",
    "Several supervised learning classification models can handle null or missing values to varying degrees. Here are some of the main classification models that are relatively tolerant of null values:\n",
    "Decision Trees and Random Forests\n",
    "Decision trees and random forest models are generally quite tolerant of missing values:\n",
    "During training, these models can work around missing values by using surrogate splits.\n",
    "For prediction, there are strategies like sending samples with missing values down both branches and averaging the results.\n",
    "Random forests in particular tend to be robust to missing data, as the ensemble nature helps mitigate issues with individual trees1.\n",
    "Naive Bayes\n",
    "Naive Bayes classifiers can handle missing values naturally:\n",
    "For categorical features, missing values can be treated as a separate category.\n",
    "For numerical features, missing values can be ignored when calculating means and variances.\n",
    "This makes Naive Bayes models quite tolerant of null values without requiring imputation2.\n",
    "K-Nearest Neighbors (KNN)\n",
    "KNN can work with missing data by using:\n",
    "Partial distance calculations that ignore missing features\n",
    "Imputation of missing values based on nearest neighbors\n",
    "While not inherently null-tolerant, KNN can be adapted to handle missing data reasonably well3.\n",
    "Support Vector Machines (SVM)\n",
    "SVMs don't directly handle missing values, but can be made more robust by:\n",
    "Using kernels that can handle missing data\n",
    "Imputing missing values before training\n",
    "With appropriate preprocessing, SVMs can work effectively even with some missing data1.\n",
    "Gradient Boosting Models\n",
    "Gradient boosting models like XGBoost and LightGBM have built-in methods for handling missing values:\n",
    "They can learn the best direction to take for missing values at each split.\n",
    "This allows them to handle missing data both during training and prediction without explicit imputation1.\n",
    "While these models can work with missing data, it's generally recommended to investigate the reason for missing values and consider imputation strategies where appropriate. The performance impact of missing data can vary depending on the specific dataset and problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtAZJZQsurF0"
   },
   "outputs": [],
   "source": [
    "px.violin(bureau, x=\"credit_day_overdue\").show()\n",
    "bureau.to_pandas().value_counts(\"credit_day_overdue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrPJYayqurF0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfdxdzKEurF0"
   },
   "outputs": [],
   "source": [
    "# pos_cash_balance: DataFrame = create_formatted_df(\"POS_CASH_balance.csv\")\n",
    "corr = pos_cash_balance.select(cs.by_dtype(pl.NUMERIC_DTYPES)).to_pandas().corr()\n",
    "features = pos_cash_balance.select(cs.by_dtype(pl.NUMERIC_DTYPES)).to_pandas().columns\n",
    "clear(pos_cash_balance)\n",
    "\n",
    "mask = np.tril(np.ones_like(corr, dtype=bool))\n",
    "masked_corr = corr.where(mask)\n",
    "masked_corr.columns = features\n",
    "masked_corr.index = features\n",
    "\n",
    "styled_corr = masked_corr.style.background_gradient(cmap=\"GnBu\").format(\"{:.3f}\")\n",
    "styled_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8ch3soxurF0"
   },
   "outputs": [],
   "source": [
    "# Print all columns in training set\n",
    "train_desc = description.filter(pl.col(\"table\") == \"application_{train|test}.csv\")[\n",
    "    [\"row\", \"description\"]\n",
    "].to_pandas()\n",
    "for row in range(len(train_desc)):\n",
    "    print(train_desc.iloc[row, 0], \": \", train_desc.iloc[row, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWPMlx44uwMK"
   },
   "outputs": [],
   "source": [
    "# for all variables in training set\n",
    "# Calculate correlation of target with variable x.\n",
    "# if absolute value of corr is >.3 and <.7, add to some correlation bucket\n",
    "# elif absolute value is >.7, add to strong correlation bucket\n",
    "# else put variable in no correlation bucket\n",
    "\n",
    "\n",
    "def group_correlations(df, feature_of_interest):\n",
    "    no_corr = dict()\n",
    "    weak_corr = dict()\n",
    "    strong_corr = dict()\n",
    "\n",
    "    for feature in df.select(cs.numeric()).columns:\n",
    "        if feature == feature_of_interest:\n",
    "            continue\n",
    "\n",
    "        # pg.partial_corr(data=application_train.to_pandas(), x='target', y='amt_goods_price', covar='target')#['r'].values[0]\n",
    "\n",
    "        corr_df = df[[feature_of_interest, feature]]\n",
    "        # corr=pg.partial_corr(data=application_train.to_pandas(), x=feature_of_interest, y=feature, covar='target')#['r'].values[0]\n",
    "        corr = corr_df.to_pandas().corr().iloc[0, 1]\n",
    "        if np.abs(corr) >= 0.7:\n",
    "            strong_corr[feature] = corr\n",
    "        elif np.abs(corr) <= 0.3:\n",
    "            no_corr[feature] = corr\n",
    "        else:\n",
    "            weak_corr[feature] = corr\n",
    "\n",
    "    index = [feature_of_interest]\n",
    "    strong_corr = pd.DataFrame(\n",
    "        data=strong_corr, index=index\n",
    "    )  # columns=strong_corr.keys(), )\n",
    "    weak_corr = pd.DataFrame(data=weak_corr, index=index)\n",
    "    no_corr = pd.DataFrame(data=no_corr, index=index)\n",
    "    return strong_corr, weak_corr, no_corr\n",
    "\n",
    "\n",
    "strong, weak, no = group_correlations(application_train, \"target\")\n",
    "\n",
    "no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bu5MURZquwMK"
   },
   "outputs": [],
   "source": [
    "# partial correlation\n",
    "pg.partial_corr(\n",
    "    data=application_train.to_pandas(), x=\"target\", y=\"amt_goods_price\", covar=\"target\"\n",
    ")[\"r\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMxKlexQuwMK"
   },
   "source": [
    "#### NB: The important point is for BorutaPy, multicollinearity should be removed before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ltLgOfsuwMK"
   },
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Set x to only features that have no null values\n",
    "x = application_train[x_features_no_nulls].drop(\"target\")  # .to_pandas()\n",
    "y = pl.DataFrame(application_train[\"target\"])  # .to_pandas()\n",
    "\n",
    "# Conduct PCA to remove multicolinearity from training set\n",
    "pca = PCA(n_components=len(x.columns))  # , svd_solver='full')\n",
    "pca.fit_transform(x)\n",
    "\n",
    "\n",
    "x_features_no_nulls = list(\n",
    "    null_df.filter(pl.col(\"null_count\") == 0).select(\"features\").to_series()\n",
    ")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "boruta = BorutaPy(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    # n_estimators=54,  # \"auto\",\n",
    "    max_iter=100,\n",
    ")\n",
    "boruta.fit(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yO8wLGbwuwMK"
   },
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(x_train.columns, columns=[\"features\"])\n",
    "feature_df[\"rank\"] = boruta.ranking_\n",
    "feature_df[\"included_features\"] = boruta.support_\n",
    "# feature_df.with_columns(boruta.ranking_).alias(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZP1EBQ5vuwML"
   },
   "outputs": [],
   "source": [
    "# Full training set correlations\n",
    "\n",
    "# pos_cash_balance: DataFrame = create_formatted_df(\"POS_CASH_balance.csv\")\n",
    "corr = application_train.select(cs.by_dtype(pl.NUMERIC_DTYPES)).to_pandas().corr()\n",
    "features = application_train.select(cs.by_dtype(pl.NUMERIC_DTYPES)).to_pandas().columns\n",
    "# clear(pos_cash_balance)\n",
    "\n",
    "mask = np.tril(np.ones_like(corr, dtype=bool))\n",
    "masked_corr = corr.where(mask)\n",
    "masked_corr.columns = features\n",
    "masked_corr.index = features\n",
    "\n",
    "styled_corr = masked_corr.style.background_gradient(cmap=\"GnBu\").format(\"{:.3f}\")\n",
    "styled_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq9OKy72uwML"
   },
   "outputs": [],
   "source": [
    "# Setting numeric & categorical features for further analysis\n",
    "response = \"target\"\n",
    "# cat_feats = [c for c in x.select(cs.string())]\n",
    "# bin_feats = [c for c in x.columns if '_bin' in c]\n",
    "# cat_feats = cat_feats + bin_feats˛\n",
    "num_feats = [c for c in x.select(cs.numeric())]\n",
    "\n",
    "# x_train, x_test, y_train, y_test\n",
    "x_train = x_train.to_pandas()\n",
    "x_test = x_test.to_pandas()\n",
    "y_train = y_train.to_pandas()\n",
    "y_test = y_test.to_pandas()\n",
    "\n",
    "\n",
    "# x[num_feats] = x[num_feats].astype('float')\n",
    "# x[cat_feats] = x[cat_feats].astype('object')\n",
    "\n",
    "# x.replace(-1, np.nan, inplace=True)\n",
    "#!pip install autofeatselect\n",
    "from autofeatselect import CorrelationCalculator, FeatureSelector, AutoFeatureSelect\n",
    "\n",
    "# Create AutoFeatureSelect class\n",
    "feat_selector = AutoFeatureSelect(\n",
    "    modeling_type=\"classification\",\n",
    "    X_train=x_train,\n",
    "    y_train=y_train,\n",
    "    X_test=x_test,\n",
    "    y_test=y_test,\n",
    "    numeric_columns=num_feats,\n",
    "    categorical_columns=[],  # cat_feats,\n",
    "    seed=24,\n",
    ")\n",
    "\n",
    "# Detect Correlated Features\n",
    "corr_features = feat_selector.calculate_correlated_features(\n",
    "    static_features=None, num_threshold=0.9, cat_threshold=0.9\n",
    ")\n",
    "# Drop Correlated Features\n",
    "feat_selector.drop_correlated_features()\n",
    "\n",
    "# Determine Selection Methods to Apply\n",
    "# Options: 'lgbm', 'xgb', 'rf','perimp', 'rfecv', 'boruta', 'lassocv'\n",
    "# Note: Hyperparameters of all methods can be changed\n",
    "selection_methods = [\"lgbm\", \"xgb\", \"rf\", \"perimp\", \"rfecv\", \"boruta\"]\n",
    "final_importance_df = feat_selector.apply_feature_selection(\n",
    "    selection_methods=selection_methods,\n",
    "    lgbm_hyperparams=None,\n",
    "    xgb_hyperparams=None,\n",
    "    rf_hyperparams=None,\n",
    "    lassocv_hyperparams=None,\n",
    "    perimp_hyperparams=None,\n",
    "    rfecv_hyperparams=None,\n",
    "    boruta_hyperparams=None,\n",
    ")\n",
    "\n",
    "# Print Results\n",
    "final_importance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PY9ycwd1uwML"
   },
   "outputs": [],
   "source": [
    "from autofeatselect import CorrelationCalculator, FeatureSelector, AutoFeatureSelect\n",
    "\n",
    "# Static features will not be removed even if they are correlated with other features.\n",
    "static_features = [\"sk_id_curr\"]\n",
    "\n",
    "# Detect correlated features\n",
    "corr_df_num, num_remove_list = CorrelationCalculator.numeric_correlations(\n",
    "    application_train,\n",
    "    features=application_train.columns,\n",
    "    static_features=static_features,\n",
    "    threshold=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zko0QtI0uwML"
   },
   "source": [
    "#### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec7vqBX0uwMM"
   },
   "outputs": [],
   "source": [
    "x1=application_train.select(feature_set1)\n",
    "\n",
    "# scale data\n",
    "scaled_x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# x_features_no_nulls = list(null_df.filter(pl.col(\"null_count\") == 0).select(\"features\").to_series())\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    scaled_x, y, test_size=0.2, stratify=y\n",
    ")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "y_train=y_train.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jer_jW_FuwMM"
   },
   "outputs": [],
   "source": [
    "# Specify the different options for hyperparameters\n",
    "grid = {\n",
    "    \"C\": np.logspace(-3, 3, 5),\n",
    "    # \"penalty\": [\"l1\", \"l2\", None],\n",
    "    \"solver\": [\"saga\", \"newton-cholesky\", \"liblinear\", \"sag\", \"saga\", \"lbfgs\"],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "    \"tol\": np.linspace(0, 0.5, 10),\n",
    "}\n",
    "\n",
    "regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "regression_params = skopt_bayesian_search(\n",
    "    regression, x_train, y_train, grid\n",
    ")  # ,np=True)\n",
    "\n",
    "# tuned_regression = LogisticRegression(**regression_params).fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(regression_params)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
